{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc4ab430",
   "metadata": {},
   "source": [
    "# OpenAI Responses API - Data and Connectors Demo\n",
    "\n",
    "This notebook demonstrates the powerful **File Search** capability of the OpenAI Responses API, which allows models to retrieve information from your uploaded documents through semantic and keyword search.\n",
    "\n",
    "The File Search tool offers several key advantages:\n",
    "- **Semantic search** - Find relevant information by meaning, not just keywords\n",
    "- **Vector store integration** - Scalable knowledge base management\n",
    "- **Automatic citations** - Get references to source documents in responses\n",
    "- **Hosted solution** - No need to implement search infrastructure yourself\n",
    "- **Retrieval customization** - Control search results, filtering, and metadata\n",
    "- **Multiple file formats** - Support for PDFs, docs, code files, and more\n",
    "\n",
    "This notebook walks through setting up vector stores, uploading documents, and using file search with the Responses API to create AI applications that can reason over your data.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c97ec0",
   "metadata": {},
   "source": [
    "### Setup and Authentication\n",
    "\n",
    "First, we need to set up our OpenAI API credentials. We'll use `getpass` to securely input the API key without exposing it in the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57f21995",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ca7137",
   "metadata": {},
   "source": [
    "Now we'll initialize the OpenAI client and import the additional libraries we'll need for file handling and requests.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cba4bc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a282edd",
   "metadata": {},
   "source": [
    "### Setting Up File Search: Vector Stores and File Upload\n",
    "\n",
    "Before we can use file search with the Responses API, we need to create a knowledge base. This involves three key steps:\n",
    "\n",
    "1. **Upload files** to the OpenAI File API\n",
    "2. **Create a vector store** to organize our knowledge base\n",
    "3. **Add files to the vector store** for semantic search\n",
    "\n",
    "Let's start by creating a helper function that can handle both local files and URLs, similar to the example in the OpenAI documentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698db8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_file(client, file_path):\n",
    "    \"\"\"\n",
    "    Upload a file to OpenAI's File API.\n",
    "    Supports both local file paths and URLs.\n",
    "    \n",
    "    Args:\n",
    "        client: OpenAI client instance\n",
    "        file_path: Path to local file or URL to download from\n",
    "        \n",
    "    Returns:\n",
    "        str: File ID from OpenAI\n",
    "    \"\"\"\n",
    "    if file_path.startswith(\"http://\") or file_path.startswith(\"https://\"):\n",
    "        # Download the file content from the URL\n",
    "        print(f\"Downloading file from URL: {file_path}\")\n",
    "        response = requests.get(file_path)\n",
    "        file_content = BytesIO(response.content)\n",
    "        file_name = file_path.split(\"/\")[-1]\n",
    "        file_tuple = (file_name, file_content)\n",
    "        result = client.files.create(\n",
    "            file=file_tuple,\n",
    "            purpose=\"assistants\"\n",
    "        )\n",
    "    else:\n",
    "        # Handle local file path\n",
    "        print(f\"Uploading local file: {file_path}\")\n",
    "        with open(file_path, \"rb\") as file_content:\n",
    "            result = client.files.create(\n",
    "                file=file_content,\n",
    "                purpose=\"assistants\"\n",
    "            )\n",
    "    \n",
    "    print(f\"File uploaded successfully. File ID: {result.id}\")\n",
    "    return result.id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6699f21",
   "metadata": {},
   "source": [
    "Now let's upload your local PDF file about embeddings from the data folder. This will serve as our knowledge base for testing file search functionality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "55a2de22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading local file: /home/chris/Code/AI Makerspace/Events/OpenAI Responses API/data/Embedding-Based.pdf\n",
      "File uploaded successfully. File ID: file-WrMJNKgKCN71PzTs2BQrE4\n"
     ]
    }
   ],
   "source": [
    "# Upload your local PDF file from the data folder\n",
    "file_id = create_file(client, \"/home/chris/Code/AI Makerspace/Events/OpenAI Responses API/data/Embedding-Based.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305ecc16",
   "metadata": {},
   "source": [
    "Next, we need to create a **vector store**. Think of this as a container that organizes your uploaded files for semantic search. The vector store will automatically process your documents and create embeddings for efficient retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "17983ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store created successfully!\n",
      "Vector Store ID: vs_68c1aea8ca8081918962708d7420ffde\n",
      "Name: AI_Makerspace_Knowledge_Base\n",
      "Status: completed\n"
     ]
    }
   ],
   "source": [
    "# Create a vector store for our knowledge base\n",
    "vector_store = client.vector_stores.create(\n",
    "    name=\"AI_Makerspace_Knowledge_Base\"\n",
    ")\n",
    "\n",
    "print(f\"Vector store created successfully!\")\n",
    "print(f\"Vector Store ID: {vector_store.id}\")\n",
    "print(f\"Name: {vector_store.name}\")\n",
    "print(f\"Status: {vector_store.status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf5e7f7",
   "metadata": {},
   "source": [
    "Now we'll add our uploaded file to the vector store. This step tells OpenAI to process the document and make it available for semantic search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1fa8d3eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File added to vector store successfully!\n",
      "File ID: file-WrMJNKgKCN71PzTs2BQrE4\n",
      "Status: in_progress\n",
      "Created at: 1757523628\n"
     ]
    }
   ],
   "source": [
    "# Add the uploaded file to our vector store\n",
    "result = client.vector_stores.files.create(\n",
    "    vector_store_id=vector_store.id,\n",
    "    file_id=file_id\n",
    ")\n",
    "\n",
    "print(f\"File added to vector store successfully!\")\n",
    "print(f\"File ID: {result.id}\")\n",
    "print(f\"Status: {result.status}\")\n",
    "print(f\"Created at: {result.created_at}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09f55aa",
   "metadata": {},
   "source": [
    "We need to wait for the file processing to complete before we can use it for search. Let's check the status and wait until it's ready. The file needs to be in `completed` status for search to work properly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8201da01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File statuses: ['completed']\n",
      "✅ All files processed successfully!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def wait_for_file_processing(client, vector_store_id, max_wait_time=300):\n",
    "    \"\"\"\n",
    "    Wait for all files in a vector store to be processed.\n",
    "    \n",
    "    Args:\n",
    "        client: OpenAI client instance\n",
    "        vector_store_id: ID of the vector store to check\n",
    "        max_wait_time: Maximum time to wait in seconds (default: 5 minutes)\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if all files are completed, False if timeout\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    while time.time() - start_time < max_wait_time:\n",
    "        # Check the status of files in the vector store\n",
    "        files = client.vector_stores.files.list(vector_store_id=vector_store_id)\n",
    "        \n",
    "        statuses = [file.status for file in files.data]\n",
    "        print(f\"File statuses: {statuses}\")\n",
    "        \n",
    "        if all(status == \"completed\" for status in statuses):\n",
    "            print(\"✅ All files processed successfully!\")\n",
    "            return True\n",
    "        elif any(status == \"failed\" for status in statuses):\n",
    "            print(\"❌ One or more files failed to process\")\n",
    "            return False\n",
    "        else:\n",
    "            print(\"⏳ Still processing... waiting 10 seconds\")\n",
    "            time.sleep(10)\n",
    "    \n",
    "    print(\"⏰ Timeout waiting for file processing\")\n",
    "    return False\n",
    "\n",
    "# Wait for our file to be processed\n",
    "wait_for_file_processing(client, vector_store.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dfacb6",
   "metadata": {},
   "source": [
    "### Basic File Search with the Responses API\n",
    "\n",
    "Now that our knowledge base is set up, we can use the **file search** tool with the Responses API! This is where the magic happens - the model can automatically search through your documents and provide answers with citations.\n",
    "\n",
    "Key features of file search:\n",
    "- **Automatic tool calling** - The model decides when to search your files\n",
    "- **Semantic understanding** - Finds relevant information by meaning, not just keywords  \n",
    "- **Source citations** - Get references to specific documents in the response\n",
    "- **Multiple output types** - Both search calls and final messages with citations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5b5d2c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Response from file search:\n",
      "==================================================\n",
      "Dense (single‑vector) embeddings have a fundamental capacity limit: because their representation power is bounded by the embedding dimension, they cannot realize all possible top‑k relevance combinations; for any fixed dimension d there exist queries/relevance patterns they will fail to return, no matter how they’re trained . This limitation can be formalized via sign‑rank: some relevance matrices require higher dimensionality than the model has, so dense embeddings cannot capture them exactly . As tasks demand more combinations (e.g., instruction‑based retrieval), the dimensionality needed grows rapidly and becomes impractical at scale .\n"
     ]
    }
   ],
   "source": [
    "# Use file search with the Responses API\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-5\",\n",
    "    input=\"What is the main problem with dense vector embeddings?\",\n",
    "    tools=[{\n",
    "        \"type\": \"file_search\",\n",
    "        \"vector_store_ids\": [vector_store.id]\n",
    "    }]\n",
    ")\n",
    "\n",
    "print(\"🔍 Response from file search:\")\n",
    "print(\"=\" * 50)\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b151b393",
   "metadata": {},
   "source": [
    "Let's examine the full response structure to understand what file search returns. The response contains multiple output items including the search call details and the final message with citations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e31037ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Full Response Analysis:\n",
      "==================================================\n",
      "\n",
      "📋 Output Item 1:\n",
      "   Type: reasoning\n",
      "   ID: rs_68c1b00540f081a081e58d4e59bff7ab047aaa5f6e571614\n",
      "\n",
      "📋 Output Item 2:\n",
      "   Type: file_search_call\n",
      "   ID: fs_68c1b008dea081a0a679634f7078f796047aaa5f6e571614\n",
      "   Status: completed\n",
      "   Queries: ['What is the main problem with dense vector embeddings?', 'limitations of dense vector embeddings main problem', 'dense embeddings problem opacity interpretability sparsity', 'dense vs sparse embeddings advantages disadvantages', 'What is the problem with dense vector representations in NLP?']\n",
      "\n",
      "📋 Output Item 3:\n",
      "   Type: reasoning\n",
      "   ID: rs_68c1b00b202881a0ace83f23d5e7eb18047aaa5f6e571614\n",
      "\n",
      "📋 Output Item 4:\n",
      "   Type: message\n",
      "   ID: msg_68c1b01ec16881a0b1554faa33adee5c047aaa5f6e571614\n",
      "   Role: assistant\n",
      "   Content items: 1\n",
      "   📚 Found 3 citations\n",
      "      Citation 1: Embedding-Based.pdf\n",
      "      Citation 2: Embedding-Based.pdf\n",
      "      Citation 3: Embedding-Based.pdf\n"
     ]
    }
   ],
   "source": [
    "# Examine the full response structure\n",
    "print(\"📊 Full Response Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, output_item in enumerate(response.output):\n",
    "    print(f\"\\n📋 Output Item {i + 1}:\")\n",
    "    print(f\"   Type: {output_item.type}\")\n",
    "    print(f\"   ID: {output_item.id}\")\n",
    "    \n",
    "    if output_item.type == \"file_search_call\":\n",
    "        print(f\"   Status: {output_item.status}\")\n",
    "        print(f\"   Queries: {output_item.queries}\")\n",
    "        \n",
    "    elif output_item.type == \"message\":\n",
    "        print(f\"   Role: {output_item.role}\")\n",
    "        print(f\"   Content items: {len(output_item.content)}\")\n",
    "        \n",
    "        # Check for citations in the message content\n",
    "        for content_item in output_item.content:\n",
    "            if hasattr(content_item, 'annotations') and content_item.annotations:\n",
    "                print(f\"   📚 Found {len(content_item.annotations)} citations\")\n",
    "                for j, annotation in enumerate(content_item.annotations):\n",
    "                    if annotation.type == \"file_citation\":\n",
    "                        print(f\"      Citation {j + 1}: {annotation.filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d4765a",
   "metadata": {},
   "source": [
    "### Including Search Results in the Response\n",
    "\n",
    "By default, the file search call doesn't return the actual search results - only the final answer with citations. However, you can include the search results using the `include` parameter. This is useful for understanding what information was retrieved and how the model used it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "646a5ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Response with search results:\n",
      "==================================================\n",
      "Here are the main limitations practitioners run into with dense, single‑vector embeddings:\n",
      "\n",
      "- Finite representational capacity tied to dimension d: For any fixed d, there exist relevance patterns (top‑k sets) that a single-vector embedder simply cannot realize—no amount of training data fixes this. This follows from sign‑rank bounds on the relevance (qrel) matrix and yields an intrinsic ceiling on what a d‑dimensional model can retrieve exactly .\n",
      "\n",
      "- Breakdowns as combinations grow: As tasks demand returning many different combinations of relevant documents (e.g., instruction‑following or reasoning queries that connect previously unrelated items), dense retrievers hit these capacity limits. Empirically, even state‑of‑the‑art models struggle on LIMIT, a simple dataset constructed to stress such combinations, with recall remaining low despite trivial content  . The “dense” qrel pattern that maximizes combinations is especially damaging across models .\n",
      "\n",
      "- You can’t just scale a bit to escape it: Performance improves with higher embedding dimensions, but representing all necessary combinations would require infeasibly large d for realistic corpora. This trade‑off is explicit both in theory and in experiments showing a critical point where increasing corpus/combination complexity outpaces the model’s dimensional capacity  .\n",
      "\n",
      "- Single‑vector bottleneck versus more expressive retrievers: Encoding an entire query or document into one vector limits expressiveness. Multi‑vector late‑interaction models and sparse lexical methods (effectively very high‑dimensional) avoid part of this bottleneck and fare much better on LIMIT, though with their own trade‑offs. Cross‑encoders don’t share the same dimensional limits but are too expensive for first‑stage retrieval at scale  .\n",
      "\n",
      "- Degradation at large index sizes (“curse” for low‑dimensional dense IR): Prior work shows smaller‑dimension dense models produce more false positives as the corpus grows; dense low‑dimensional IR degrades with large indexes, reinforcing the practical impact of limited dimensionality .\n",
      "\n",
      "- Benchmarks can mask the problem: Common IR benchmarks cover a narrow slice of possible queries and can be overfit, hiding these limits. On LIMIT, performance is poorly correlated with BEIR, underscoring that standard scores may not reveal true capacity constraints  .\n",
      "\n",
      "- Non‑metric behavior of common similarities: Cosine similarity is not a metric, so triangle‑inequality–based intuitions and some theoretical guarantees or pruning strategies don’t apply as they might in metric spaces, complicating analysis and some indexing optimizations .\n",
      "\n",
      "In short, dense single‑vector embeddings are powerful but fundamentally capacity‑limited by embedding dimension; as retrieval tasks require representing more diverse and combinatorial notions of relevance, these limits become unavoidable without switching to more expressive architectures or adding expensive reranking stages  .\n"
     ]
    }
   ],
   "source": [
    "# Include search results in the response\n",
    "response_with_results = client.responses.create(\n",
    "    model=\"gpt-5\",\n",
    "    input=\"What are the key limitations of dense vector embeddings?\",\n",
    "    tools=[{\n",
    "        \"type\": \"file_search\",\n",
    "        \"vector_store_ids\": [vector_store.id]\n",
    "    }],\n",
    "    include=[\"file_search_call.results\"]\n",
    ")\n",
    "\n",
    "print(\"🔍 Response with search results:\")\n",
    "print(\"=\" * 50)\n",
    "print(response_with_results.output_text)\n",
    "\n",
    "# Examine the search results\n",
    "for output_item in response_with_results.output:\n",
    "    if output_item.type == \"file_search_call\" and hasattr(output_item, 'search_results'):\n",
    "        if output_item.search_results:\n",
    "            print(f\"\\n📄 Search Results Found:\")\n",
    "            print(f\"Number of results: {len(output_item.search_results)}\")\n",
    "            \n",
    "            for i, result in enumerate(output_item.search_results):\n",
    "                print(f\"\\nResult {i + 1}:\")\n",
    "                print(f\"  Score: {result.score}\")\n",
    "                print(f\"  Content preview: {result.content[:200]}...\")\n",
    "        else:\n",
    "            print(\"\\n📄 No search results returned (search_results is None)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395be437",
   "metadata": {},
   "source": [
    "### Retrieval Customization\n",
    "\n",
    "The file search tool offers several customization options to optimize performance and results:\n",
    "\n",
    "1. **Limiting results** - Control the number of search results to reduce tokens and latency\n",
    "2. **Metadata filtering** - Filter search results based on file attributes\n",
    "3. **Search quality vs. performance** - Balance between comprehensive results and response speed\n",
    "\n",
    "Let's explore these customization options:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df98e54",
   "metadata": {},
   "source": [
    "#### Limiting the Number of Results\n",
    "\n",
    "By default, file search may retrieve many results. You can limit this using `max_num_results` to reduce token usage and improve response time, though this may impact answer quality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "56e82112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Limited Results Response:\n",
      "==================================================\n",
      "The core problem is limited expressivity: a single fixed‑dimensional dense vector can’t encode all the combinations of document–query relevance you may need. For any chosen dimension, there exist top‑k relevance patterns that cannot be realized—no matter how you train the model—so as corpora and tasks grow, dense embeddings hit a hard capacity limit unless you dramatically increase dimensionality . In fact, theoretical bounds show the required dimensionality can be much larger than what’s practical for real IR problems, which leads to recall errors and false positives at scale .\n"
     ]
    }
   ],
   "source": [
    "# Limit the number of search results\n",
    "response_limited = client.responses.create(\n",
    "    model=\"gpt-5\",\n",
    "    input=\"What is the main problem with dense vector embeddings?\",\n",
    "    tools=[{\n",
    "        \"type\": \"file_search\",\n",
    "        \"vector_store_ids\": [vector_store.id],\n",
    "        \"max_num_results\": 2  # Limit to 2 results for faster, more focused responses\n",
    "    }],\n",
    "    include=[\"file_search_call.results\"]\n",
    ")\n",
    "\n",
    "print(\"🎯 Limited Results Response:\")\n",
    "print(\"=\" * 50)\n",
    "print(response_limited.output_text)\n",
    "\n",
    "# Show how many results were actually used\n",
    "for output_item in response_limited.output:\n",
    "    if output_item.type == \"file_search_call\":\n",
    "        if hasattr(output_item, 'search_results') and output_item.search_results:\n",
    "            print(f\"\\n📊 Used {len(output_item.search_results)} search results (limited to 2)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfae0f6a",
   "metadata": {},
   "source": [
    "#### Practical Example: Adding Local Files to Your Knowledge Base\n",
    "\n",
    "Let's demonstrate how to add local files from your project to create a more comprehensive knowledge base. We'll check if there are any PDF files in the data directory and add them to our vector store.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0f1bd3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Found 2 PDF files in ./data:\n",
      "   - Embedding-Based.pdf\n",
      "   - Harnessing Embeddings.pdf\n",
      "\n",
      "📤 Uploading Embedding-Based.pdf...\n",
      "Uploading local file: ./data/Embedding-Based.pdf\n",
      "File uploaded successfully. File ID: file-EunX48ia7QfQNNhXppgJgK\n",
      "✅ Added to vector store successfully!\n",
      "\n",
      "⏳ Waiting for new files to be processed...\n",
      "File statuses: ['in_progress', 'completed']\n",
      "⏳ Still processing... waiting 10 seconds\n",
      "File statuses: ['completed', 'completed']\n",
      "✅ All files processed successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# Check for local PDF files in the data directory\n",
    "data_dir = \"./data\"\n",
    "pdf_files = []\n",
    "\n",
    "if os.path.exists(data_dir):\n",
    "    pdf_files = glob.glob(os.path.join(data_dir, \"*.pdf\"))\n",
    "    print(f\"📁 Found {len(pdf_files)} PDF files in {data_dir}:\")\n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"   - {os.path.basename(pdf_file)}\")\n",
    "else:\n",
    "    print(f\"📁 Data directory {data_dir} not found\")\n",
    "\n",
    "# Upload and add local files to the vector store\n",
    "local_file_ids = []\n",
    "for pdf_file in pdf_files[:1]:  # Let's add just the first PDF to avoid too much processing time\n",
    "    try:\n",
    "        print(f\"\\n📤 Uploading {os.path.basename(pdf_file)}...\")\n",
    "        local_file_id = create_file(client, pdf_file)\n",
    "        local_file_ids.append(local_file_id)\n",
    "        \n",
    "        # Add to vector store\n",
    "        result = client.vector_stores.files.create(\n",
    "            vector_store_id=vector_store.id,\n",
    "            file_id=local_file_id\n",
    "        )\n",
    "        print(f\"✅ Added to vector store successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing {pdf_file}: {e}\")\n",
    "\n",
    "if local_file_ids:\n",
    "    print(f\"\\n⏳ Waiting for new files to be processed...\")\n",
    "    wait_for_file_processing(client, vector_store.id)\n",
    "else:\n",
    "    print(\"\\n💡 No local files were added. You can place PDF files in the ./data directory to test with your own documents!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfdd3ac",
   "metadata": {},
   "source": [
    "### Multi-Document Search and Comparison\n",
    "\n",
    "Now that we have multiple documents in our knowledge base, we can ask questions that require searching across different sources. This demonstrates the power of semantic search for complex queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9f992e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔬 Complex Multi-Document Query:\n",
      "============================================================\n",
      "Below is a concise map of the main AI R&D approaches and methodologies discussed in the documents, with their benefits and trade‑offs.\n",
      "\n",
      "1) Theoretical analysis of model limits\n",
      "- What it is: Use tools from communication complexity and sign‑rank to prove lower bounds on what single‑vector (dense) embedding models can represent in top‑k retrieval; relates but contrasts with geometric intuitions like order‑k Voronoi regions. Also clarifies why triangle‑inequality arguments don’t apply when using cosine similarity (non‑metric)  .\n",
      "- Benefits:\n",
      "  - Gives rigorous, architecture‑agnostic limits for single‑vector embeddings across modalities: for a fixed embedding dimension d, some top‑k combinations simply cannot be returned by any query  .\n",
      "  - Guides system design by explaining why scaling tasks that require many combinatorial combinations will eventually hit representation ceilings .\n",
      "- Trade‑offs:\n",
      "  - Results target the dominant single‑vector paradigm; extensions to other architectures (e.g., multi‑vector) are left for future work .\n",
      "\n",
      "2) Empirical stress‑testing and dataset construction\n",
      "- What it is: “Best‑case” empirical tests that directly optimize vectors on the test set (“free embeddings”), plus construction of the LIMIT dataset to stress top‑k combination capacity with simple queries; ablations over qrel graph density show why dense combination patterns are uniquely hard  .\n",
      "- Benefits:\n",
      "  - Confirms the theoretical bounds in practice and isolates dimensionality as the critical bottleneck, even on trivial language tasks; models struggle markedly as combinations grow, and performance rises with larger dimensions .\n",
      "  - Shows the difficulty isn’t mere domain shift: fine‑tuning on the train split barely helps; overfitting the test split can “memorize” small setups, underscoring real capacity limits in practical embedders .\n",
      "  - Reveals weak correlation between LIMIT and popular benchmarks (e.g., BEIR/MTEB), cautioning against overreliance on a single benchmark family and highlighting overfitting risks .\n",
      "- Trade‑offs:\n",
      "  - Stress tests surface worst‑case behavior; they are invaluable for understanding limits but are intentionally hard by design .\n",
      "\n",
      "3) Retriever architecture choices\n",
      "- Single‑vector dense embeddings (standard “dense retrieval”)\n",
      "  - What it is: One vector per sequence; fast ANN search; widely used for generalization and instruction‑following retrieval .\n",
      "  - Benefits: Scalable first‑stage retrieval; strong generalization across domains and tasks when queries are not highly combinatorial .\n",
      "  - Limits: Fundamentally dimension‑bounded; as tasks require connecting many otherwise unrelated documents (e.g., via logical operators), dense embedders miss combinations no matter the query; bigger d helps but cannot cover “all” combinations at realistic scales  .\n",
      "- Multi‑vector late‑interaction models (e.g., ColBERT‑style)\n",
      "  - What it is: Multiple vectors per sequence with MaxSim to increase expressiveness .\n",
      "  - Benefits: Significantly outperforms single‑vector models on LIMIT, indicating higher combinational capacity .\n",
      "  - Trade‑offs: More compute/memory than single‑vector; less explored for instruction‑following or reasoning‑heavy retrieval tasks .\n",
      "- Cross‑encoders / LLM rerankers\n",
      "  - What it is: Score query–document pairs jointly (often with long‑context LLMs) as a reranking stage .\n",
      "  - Benefits: Extremely accurate; a long‑context LLM reranker (Gemini 2.5 Pro) solved 100% of LIMIT‑small in one pass, bypassing single‑vector dimensional limits .\n",
      "  - Trade‑offs: Too expensive for first‑stage retrieval at scale; best used after an efficient retriever narrows candidates .\n",
      "- Sparse lexical/neural retrieval (e.g., BM25)\n",
      "  - What it is: Very high‑dimensional sparse vectors keyed to terms/features .\n",
      "  - Benefits: Avoids dense bottlenecks; near‑perfect on LIMIT while dense models fail, thanks to effectively huge dimensionality .\n",
      "  - Trade‑offs: Weaker for instruction‑following or reasoning definitions of “relevance” where lexical overlap is minimal; applying sparse methods to such tasks remains open .\n",
      "\n",
      "4) Training and optimization methods for retrievers\n",
      "- What they are: Contrastive learning with MultipleNegativesRankingLoss and large in‑batch negatives; dimensionality control via Matryoshka (MRL) and truncation experiments to study d → performance; instruction‑following finetuning (e.g., Promptriever)  .\n",
      "- Benefits:\n",
      "  - Clear evidence that higher embedding dimensionality boosts recall on combination‑heavy tasks; instruction‑diverse training may better utilize the available embedding space .\n",
      "- Trade‑offs:\n",
      "  - Even with careful finetuning, single‑vector models remain dimension‑limited; training on held‑out data doesn’t “fix” the fundamental capacity constraint, while training on test induces overfitting rather than true general solution capacity .\n",
      "\n",
      "5) Benchmarking and evaluation methodology\n",
      "- What it is: Comparison of traditional benchmarks (BEIR/MTEB) to LIMIT; analysis of qrel graph density (how queries connect documents) as a key hardness factor; caution against benchmark overfitting and misgeneralization  .\n",
      "- Benefits:\n",
      "  - Encourages designing evaluations that probe combinational generalization, not just topical similarity, and that align with instruction‑following/reasoning use cases .\n",
      "- Trade‑offs:\n",
      "  - Harder benchmarks can depress headline scores, but they reveal genuine system limits critical for robust deployments .\n",
      "\n",
      "Bottom line comparisons\n",
      "- If you need scalable first‑stage retrieval: single‑vector dense is efficient, but know its hard limit is embedding dimension; increase d and/or pair with a stronger second stage .\n",
      "- If you need higher recall on combinatorial or instruction‑defined relevance: switch to multi‑vector or add a cross‑encoder/LLM reranker; they empirically overcome dense limits at higher compute cost  .\n",
      "- If your task aligns with lexical cues: sparse methods like BM25 can excel and avoid dense bottlenecks, though they may struggle when “relevance” requires reasoning beyond overlap  .\n",
      "- For research methodology: combine theory (to know what’s impossible), targeted stress tests like LIMIT (to verify and quantify), and diversified benchmarks (to avoid overfitting blind spots)   .\n",
      "\n",
      "Finally, the documents argue these insights apply broadly across modalities for single‑vector embeddings; as tasks and instructions broaden, the same dimension‑driven limits surface, motivating more expressive architectures or multi‑stage pipelines for robust AI systems  .\n",
      "\n",
      "📚 Documents Referenced:\n",
      "==============================\n",
      "📄 Embedding-Based.pdf\n"
     ]
    }
   ],
   "source": [
    "# Ask a complex question that might require multiple documents\n",
    "complex_response = client.responses.create(\n",
    "    model=\"gpt-5\",\n",
    "    input=\"Compare and contrast different approaches to AI research and development mentioned in the documents. What are the key methodologies and their benefits?\",\n",
    "    tools=[{\n",
    "        \"type\": \"file_search\",\n",
    "        \"vector_store_ids\": [vector_store.id]\n",
    "    }],\n",
    "    include=[\"file_search_call.results\"]\n",
    ")\n",
    "\n",
    "print(\"🔬 Complex Multi-Document Query:\")\n",
    "print(\"=\" * 60)\n",
    "print(complex_response.output_text)\n",
    "\n",
    "# Show which documents were referenced\n",
    "print(\"\\n📚 Documents Referenced:\")\n",
    "print(\"=\" * 30)\n",
    "for output_item in complex_response.output:\n",
    "    if output_item.type == \"message\":\n",
    "        for content_item in output_item.content:\n",
    "            if hasattr(content_item, 'annotations') and content_item.annotations:\n",
    "                unique_files = set()\n",
    "                for annotation in content_item.annotations:\n",
    "                    if annotation.type == \"file_citation\":\n",
    "                        unique_files.add(annotation.filename)\n",
    "                \n",
    "                for filename in unique_files:\n",
    "                    print(f\"📄 {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016fc7fb",
   "metadata": {},
   "source": [
    "### Supported File Formats\n",
    "\n",
    "The file search tool supports a wide variety of file formats. Here's a summary of what you can upload:\n",
    "\n",
    "**Text and Code Files:**\n",
    "- `.py`, `.js`, `.ts`, `.java`, `.cpp`, `.c`, `.cs`, `.go`, `.php`, `.rb`, `.sh`\n",
    "- `.html`, `.css`, `.json`, `.md`, `.txt`, `.tex`\n",
    "\n",
    "**Document Formats:**\n",
    "- `.pdf` - PDF documents\n",
    "- `.doc`, `.docx` - Microsoft Word documents  \n",
    "- `.pptx` - PowerPoint presentations\n",
    "\n",
    "**Requirements:**\n",
    "- For text files, encoding must be UTF-8, UTF-16, or ASCII\n",
    "- Files are processed automatically for semantic search\n",
    "- Maximum file size and token limits apply (check OpenAI documentation for current limits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1513aa",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The OpenAI Responses API with File Search represents a powerful leap forward in building AI applications that can reason over your data. Key takeaways:\n",
    "\n",
    "✅ **Hosted solution** - No need to manage your own vector database or search infrastructure  \n",
    "✅ **Semantic search** - Find information by meaning, not just keywords  \n",
    "✅ **Automatic citations** - Get references to source documents in responses  \n",
    "✅ **Multiple file formats** - Support for PDFs, docs, code files, and more  \n",
    "✅ **Retrieval customization** - Control search results and performance  \n",
    "✅ **Easy integration** - Simple API that works seamlessly with the Responses API  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69175ab8",
   "metadata": {},
   "source": [
    "# MCP (Model Context Protocol) and Connectors\n",
    "\n",
    "In addition to file search, the OpenAI Responses API supports **MCP (Model Context Protocol)** servers and **Connectors** that give models the ability to connect to and control external services. This section demonstrates:\n",
    "\n",
    "## Two Types of External Integrations:\n",
    "\n",
    "### 1. **Connectors** \n",
    "- OpenAI-maintained MCP wrappers for popular services\n",
    "- Pre-built integrations like Google Workspace, Dropbox, Microsoft 365\n",
    "- OAuth-based authentication\n",
    "- Hosted by OpenAI with guaranteed reliability\n",
    "\n",
    "### 2. **Remote MCP Servers**\n",
    "- Third-party servers implementing the MCP protocol\n",
    "- Can be any server on the public Internet\n",
    "- Custom tools and capabilities\n",
    "- Examples: GitHub, Stripe, custom business tools\n",
    "\n",
    "## Key Benefits:\n",
    "- **Automatic tool discovery** - Models learn available tools dynamically\n",
    "- **Approval workflows** - Control what data is shared with external services  \n",
    "- **Real-time capabilities** - Access live data and perform actions\n",
    "- **Extensible** - Connect to virtually any service with an MCP server\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691cf6fb",
   "metadata": {},
   "source": [
    "⚠️  **Please obtain a Google OAuth token first!**\n",
    "\n",
    "📝 **Steps to get a token:**\n",
    "1. Go to [Google OAuth Playground](https://developers.google.com/oauthplayground/)\n",
    "2. Enter scope: `https://www.googleapis.com/auth/calendar.events`\n",
    "3. Authorize APIs and exchange for token\n",
    "4. Replace the token in the code above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4eab0968",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_OAUTH_TOKEN\"] = getpass.getpass(\"Enter your Google OAuth token: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cad697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📅 Google Calendar Response:\n",
      "==================================================\n",
      "Here’s what’s on your calendar for Friday, September 12 (times shown in your calendar’s timezone: America/Los_Angeles):\n",
      "\n",
      "- 10:00 AM–10:30 AM — GPU Engineering Meetups: Fundamentals of GPU Orchestration\n",
      "  - Location: https://luma.com/join/g-wfW1s6SgOuSmfwh\n",
      "  - Details: Overview of why GPU orchestration matters; how Ray, Horovod, and DeepSpeed work; consistency models; and real-world trade-offs. Hosted by Abi.\n",
      "  - More info: https://luma.com/event/evt-MierqIlbxNeBy87?pk=g-wfW1s6SgOuSmfwh\n",
      "  - Calendar link: https://www.google.com/calendar/event?eid=X2Nscjc4YmFkZDVpbjRzYTlkaGg3Z2pqNTg5c2pnZHEwY2xyNmFyamtlY242b3Q5ZWRsZ2cgY2hyaXNAYWxleGl1ay5jYQ\n",
      "  - Status: Busy\n",
      "\n",
      "Want me to add a reminder or pull in attendee/meeting details if available?\n"
     ]
    }
   ],
   "source": [
    "# Google Calendar Connector Example\n",
    "# Note: You'll need to obtain an OAuth access token from Google\n",
    "# For testing, use Google's OAuth 2.0 Playground: https://developers.google.com/oauthplayground/\n",
    "\n",
    "# Example OAuth scope for Google Calendar:\n",
    "# https://www.googleapis.com/auth/calendar.events\n",
    "\n",
    "def demonstrate_google_calendar_connector(token):\n",
    "    \"\"\"\n",
    "    Demonstrate Google Calendar Connector functionality.\n",
    "    This example shows how to query calendar events using the Responses API.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Query today's calendar events\n",
    "        response = client.responses.create(\n",
    "            model=\"gpt-5\",\n",
    "            tools=[{\n",
    "                \"type\": \"mcp\",\n",
    "                \"server_label\": \"google_calendar\",\n",
    "                \"connector_id\": \"connector_googlecalendar\",\n",
    "                \"authorization\": token,\n",
    "                \"require_approval\": \"never\"  # Set to \"always\" for production\n",
    "            }],\n",
    "            input=\"What events do I have on my Google Calendar for Friday, September 12th? Please provide details about each event including time, title, and any important information.\"\n",
    "        )\n",
    "        \n",
    "        print(\"📅 Google Calendar Response:\")\n",
    "        print(\"=\" * 50)\n",
    "        print(response.output_text)\n",
    "        \n",
    "        return response\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error with Google Calendar Connector: {e}\")\n",
    "        return None\n",
    "\n",
    "# Demonstrate the connector (will show setup instructions if no token provided)\n",
    "calendar_response = demonstrate_google_calendar_connector(os.environ[\"GOOGLE_OAUTH_TOKEN\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3a04f4",
   "metadata": {},
   "source": [
    "#### Advanced Google Calendar Operations\n",
    "\n",
    "The Google Calendar Connector supports several sophisticated operations that can be combined for powerful calendar-based workflows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "92c871e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Advanced Calendar Analysis:\n",
      "==================================================\n",
      "Here’s your week-at-a-glance analysis based on your primary Google Calendar for Sep 8–14, 2025 (assumed timezone: America/Los_Angeles). If your timezone or workweek differs, tell me and I’ll re-run.\n",
      "\n",
      "1) Busiest days\n",
      "- Monday (Sep 8): 1 meeting, 30 minutes\n",
      "  - 10:30–11:00 — “Chris Alexiuk and Anthony Witherspoon”\n",
      "- Friday (Sep 12): 1 meeting, 30 minutes\n",
      "  - 10:00–10:30 — “GPU Engineering Meetups: Fundamentals of GPU Orchestration”\n",
      "- Tuesday, Wednesday, Thursday: No meetings\n",
      "\n",
      "2) Potential scheduling conflicts\n",
      "- None found. No overlaps or back-to-backs. Both events are short and separated by days.\n",
      "\n",
      "3) Recommendations for optimal meeting times\n",
      "Assumptions: standard working hours 9:00–17:00 PT.\n",
      "- Best days for meetings this week (widest availability):\n",
      "  - Tuesday (Sep 9): 9:00–12:00, 13:00–17:00 completely open\n",
      "  - Wednesday (Sep 10): 9:00–12:00, 13:00–17:00 completely open\n",
      "  - Thursday (Sep 11): 9:00–12:00, 13:00–17:00 completely open\n",
      "- Good windows on days with existing events:\n",
      "  - Monday (Sep 8): 9:00–10:30, 11:00–17:00\n",
      "  - Friday (Sep 12): 9:00–10:00, 10:30–17:00\n",
      "- Actionable scheduling tips\n",
      "  - If you’re proposing times to others: offer Tue–Thu 10:00–12:00 PT or 13:30–16:00 PT for best acceptance and focus.\n",
      "  - Cluster meetings: Since you already have brief items Mon and Fri mornings, consider slotting other external/admin meetings adjacent to those blocks to keep Tue–Thu more open for deep work.\n",
      "  - Hold protected focus blocks: Place 2–3 hour focus blocks Tue–Thu (e.g., 9:30–12:00) to guard maker time without impacting any existing commitments.\n",
      "\n",
      "4) Total hours of scheduled meetings\n",
      "- 1.0 hour (two 30-minute meetings)\n",
      "\n",
      "Would you like me to:\n",
      "- Include other calendars (e.g., work, shared, resource calendars) in the analysis?\n",
      "- Re-run using your actual timezone and preferred working hours?\n",
      "\n",
      "🔍 MCP Operations Performed:\n",
      "==============================\n",
      "\n",
      "📋 MCP Call 3:\n",
      "   Tool: search_events\n",
      "   Status: ✅ Success\n",
      "   Output preview: {\"events\": [{\"id\": \"1tnut9gmegv5t0b57talo4i3ng\", \"summary\": \"Chris Alexiuk and Anthony Witherspoon\", \"location\": \"Google Meet (instructions in description)\", \"start\": \"2025-09-08T10:30:00-07:00\", \"end...\n"
     ]
    }
   ],
   "source": [
    "def demonstrate_advanced_calendar_operations(token):\n",
    "    \"\"\"\n",
    "    Demonstrate advanced Google Calendar operations including:\n",
    "    - Weekly schedule analysis\n",
    "    - Meeting conflict detection\n",
    "    - Event filtering and search\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Advanced calendar analysis query\n",
    "        response = client.responses.create(\n",
    "            model=\"gpt-5\",\n",
    "            tools=[{\n",
    "                \"type\": \"mcp\",\n",
    "                \"server_label\": \"google_calendar\",\n",
    "                \"connector_id\": \"connector_googlecalendar\", \n",
    "                \"authorization\": token,\n",
    "                \"require_approval\": \"never\",\n",
    "                # Limit to specific tools for focused functionality\n",
    "                \"allowed_tools\": [\"search_events\", \"read_event\"]\n",
    "            }],\n",
    "            input=\"\"\"Analyze my Google Calendar for this week and provide:\n",
    "            1. A summary of my busiest days\n",
    "            2. Any potential scheduling conflicts\n",
    "            3. Recommendations for optimal meeting times\n",
    "            4. Total hours of scheduled meetings\n",
    "            \n",
    "            Please be thorough in your analysis and provide actionable insights.\"\"\"\n",
    "        )\n",
    "        \n",
    "        print(\"📊 Advanced Calendar Analysis:\")\n",
    "        print(\"=\" * 50)\n",
    "        print(response.output_text)\n",
    "        \n",
    "        # Examine the MCP calls that were made\n",
    "        print(\"\\n🔍 MCP Operations Performed:\")\n",
    "        print(\"=\" * 30)\n",
    "        \n",
    "        for i, output_item in enumerate(response.output):\n",
    "            if output_item.type == \"mcp_call\":\n",
    "                print(f\"\\n📋 MCP Call {i + 1}:\")\n",
    "                print(f\"   Tool: {output_item.name}\")\n",
    "                print(f\"   Status: {'✅ Success' if not output_item.error else '❌ Error'}\")\n",
    "                if output_item.error:\n",
    "                    print(f\"   Error: {output_item.error}\")\n",
    "                else:\n",
    "                    # Show a preview of the output\n",
    "                    output_preview = str(output_item.output)[:200]\n",
    "                    print(f\"   Output preview: {output_preview}...\")\n",
    "        \n",
    "        return response\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error with advanced calendar operations: {e}\")\n",
    "        return None\n",
    "\n",
    "# Run advanced calendar analysis\n",
    "advanced_calendar_response = demonstrate_advanced_calendar_operations(os.environ[\"GOOGLE_OAUTH_TOKEN\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ddc496",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## GitMCP Server\n",
    "\n",
    "The GitMCP server provides access to documentation and code repositories through a specialized MCP interface. Unlike connectors, this is a remote MCP server that implements the Model Context Protocol for accessing various code documentation resources.\n",
    "\n",
    "**Key Features:**\n",
    "- Documentation search and retrieval\n",
    "- Code analysis and explanation\n",
    "- Technical reference access\n",
    "- Real-time documentation fetching\n",
    "- Specialized tiktoken documentation access\n",
    "\n",
    "**Server Details:**\n",
    "- **Server URL:** `https://gitmcp.io/openai/tiktoken`\n",
    "- **Authentication:** None required for public documentation\n",
    "- **Protocol:** HTTP transport\n",
    "- **Maintained by:** GitMCP (third-party)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61e0f65",
   "metadata": {},
   "source": [
    "ℹ️ **No Authentication Required!**\n",
    "\n",
    "The GitMCP server for tiktoken documentation is publicly accessible and doesn't require any authentication tokens. This makes it perfect for demonstrating MCP functionality without setup complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0edd26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No token required for GitMCP tiktoken documentation server\n",
    "print(\"✅ GitMCP server ready - no authentication required!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "65d02eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🐙 GitHub Analysis Response:\n",
      "==================================================\n",
      "TikToken is OpenAI’s fast byte pair encoding (BPE) tokenizer for mapping text to the integer tokens expected by OpenAI models, and back again.\n",
      "\n",
      "What it does\n",
      "- Converts text to tokens (integers) and back, reversibly and losslessly.\n",
      "- Works on arbitrary Unicode text via a byte-level scheme with “byte fallback,” so every byte sequence can be tokenized.\n",
      "- Uses model-specific vocabularies/merge rules so you get the same tokenization the model was trained with.\n",
      "\n",
      "How it works under the hood\n",
      "1) Special tokens\n",
      "- Certain reserved strings (e.g., <|endoftext|>, chat markers) are mapped to fixed IDs and can be handled explicitly.\n",
      "- You control whether special tokens are allowed in input using parameters like allowed_special or by using encode_ordinary/encode_with_special_tokens.\n",
      "\n",
      "2) Regex pre-tokenization (pat_str)\n",
      "- Text is split into initial chunks using a carefully designed Unicode-aware regex (pat_str) that captures common patterns: spaces, punctuation, word fragments, numbers, etc.\n",
      "- This step approximates linguistic boundaries while remaining purely rule-based.\n",
      "\n",
      "3) Byte-level BPE merges\n",
      "- Each chunk is represented as bytes. A merge table (mergeable_ranks) ranks which adjacent byte sequences to merge.\n",
      "- The algorithm repeatedly merges the highest-priority pair (lowest rank value) until no more merges are allowed, producing subword units that become token IDs.\n",
      "- If a sequence isn’t in the merge table, it falls back to individual byte tokens (ensuring full coverage).\n",
      "\n",
      "4) Decoding\n",
      "- To decode, TikToken maps token IDs back to their byte sequences, concatenates them, and decodes as UTF-8 (plus special tokens when present), reconstructing the original text.\n",
      "\n",
      "Encodings and models\n",
      "- Different OpenAI model families use different base encodings:\n",
      "  - r50k_base / p50k_base (older GPT-2/3 style)\n",
      "  - cl100k_base (GPT-3.5, GPT-4 families, most embeddings)\n",
      "  - o200k_base (newer large-context models like GPT-4o variants)\n",
      "- encoding_for_model(\"model-name\") returns the right encoding for a given model.\n",
      "\n",
      "Performance and implementation\n",
      "- Core implemented in Rust with Python bindings for speed and low memory overhead.\n",
      "- Typically 3–6× faster than comparable open-source tokenizers in benchmarks.\n",
      "\n",
      "Basic API (Python)\n",
      "- Get an encoding:\n",
      "  - import tiktoken\n",
      "  - enc = tiktoken.get_encoding(\"cl100k_base\")\n",
      "  - or enc = tiktoken.encoding_for_model(\"gpt-4o\")\n",
      "- Encode/decode:\n",
      "  - ids = enc.encode(\"hello world\")\n",
      "  - text = enc.decode(ids)\n",
      "- Special tokens control:\n",
      "  - enc.encode(\"text <|endoftext|>\", allowed_special={\"<|endoftext|>\"})\n",
      "  - enc.encode_ordinary(\"...\") ignores special handling\n",
      "  - enc.encode_with_special_tokens(\"...\") always treats specials specially\n",
      "- Count tokens:\n",
      "  - n = len(enc.encode(some_text))\n",
      "\n",
      "Extensibility\n",
      "- You can construct your own Encoding by specifying:\n",
      "  - name, pat_str (regex), mergeable_ranks (vocab/merges), special_tokens (map)\n",
      "- Or register custom encodings via the tiktoken_ext plugin mechanism so get_encoding can find them.\n",
      "\n",
      "Key takeaways\n",
      "- TikToken is a fast, byte-level BPE tokenizer tuned to match OpenAI model training tokenization.\n",
      "- It splits text with a regex, merges bytes via ranked BPE, supports special tokens, and guarantees coverage.\n",
      "- Use encoding_for_model to avoid mismatches and count tokens or prepare inputs reliably.\n"
     ]
    }
   ],
   "source": [
    "# GitMCP Server Example\n",
    "\n",
    "def demonstrate_github_mcp_server():\n",
    "    \"\"\"\n",
    "    Demonstrate GitMCP server functionality.\n",
    "    Shows how to interact with GitHub repositories using the official GitMCP server.\n",
    "    \"\"\"\n",
    "    response = client.responses.create(\n",
    "        model=\"gpt-5\",\n",
    "        tools=[{\n",
    "            \"type\": \"mcp\",\n",
    "            \"server_label\": \"gitmcp\",\n",
    "            \"server_url\": \"https://gitmcp.io/openai/tiktoken\",\n",
    "            \"allowed_tools\": [\"search_tiktoken_documentation\", \"fetch_tiktoken_documentation\"],\n",
    "            \"require_approval\": \"never\"  # Set to \"always\" for production\n",
    "        }],\n",
    "        input=\"\"\"How does TikToken work?\"\"\"\n",
    "    )\n",
    "    \n",
    "    print(\"🐙 GitHub Analysis Response:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(response.output_text)\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Demonstrate the GitMCP server\n",
    "github_response = demonstrate_github_mcp_server()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae386faf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Complete Summary: OpenAI Responses API with Data and Connectors\n",
    "\n",
    "This notebook demonstrated the full capabilities of the OpenAI Responses API for building AI applications that can access and reason over external data sources. Here's what we covered:\n",
    "\n",
    "### 🔍 File Search Capabilities\n",
    "- **Vector Store Management** - Creating and managing knowledge bases\n",
    "- **Multi-format Support** - PDFs, documents, code files, and more  \n",
    "- **Semantic Search** - Finding information by meaning, not just keywords\n",
    "- **Automatic Citations** - Getting references to source documents\n",
    "- **Retrieval Customization** - Controlling search results and performance\n",
    "\n",
    "### 🔌 MCP Servers and Connectors\n",
    "- **Google Calendar Connector** - Reading and analyzing calendar events with OAuth\n",
    "- **GitMCP** - Checking out the GitHub Docs!\n",
    "\n",
    "### 🛡️ Security and Best Practices\n",
    "- **OAuth Authentication** - Secure token management for external services\n",
    "- **Approval Controls** - Fine-grained permission management\n",
    "- **Error Recovery** - Retry logic and graceful failure handling\n",
    "- **Data Privacy** - Understanding what data is shared with external services"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
