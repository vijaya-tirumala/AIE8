{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangGraph Open Deep Research - Supervisor-Researcher Architecture\n",
    "\n",
    "In this notebook, we'll explore the **supervisor-researcher delegation architecture** for conducting deep research with LangGraph.\n",
    "\n",
    "You can visit this repository to see the original application: [Open Deep Research](https://github.com/langchain-ai/open_deep_research)\n",
    "\n",
    "Let's jump in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What We're Building\n",
    "\n",
    "This implementation uses a **hierarchical delegation pattern** where:\n",
    "\n",
    "1. **User Clarification** - Optionally asks clarifying questions to understand the research scope\n",
    "2. **Research Brief Generation** - Transforms user messages into a structured research brief\n",
    "3. **Supervisor** - A lead researcher that analyzes the brief and delegates research tasks\n",
    "4. **Parallel Researchers** - Multiple sub-agents that conduct focused research simultaneously\n",
    "5. **Research Compression** - Each researcher synthesizes their findings\n",
    "6. **Final Report** - All findings are combined into a comprehensive report\n",
    "\n",
    "![Architecture Diagram](Open%20Deep%20Research.png)\n",
    "\n",
    "This differs from a section-based approach by allowing dynamic task decomposition based on the research question, rather than predefined sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "\n",
    "You'll need API keys for Anthropic (for the LLM) and Tavily (for web search). We'll configure the system to use Anthropic's Claude Sonnet 4 exclusively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass(\"Enter your Anthropic API key: \")\n",
    "os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"Enter your Tavily API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: State Definitions\n",
    "\n",
    "The state structure is hierarchical with three levels:\n",
    "\n",
    "### Agent State (Top Level)\n",
    "Contains the overall conversation messages, research brief, accumulated notes, and final report.\n",
    "\n",
    "### Supervisor State (Middle Level)\n",
    "Manages the research supervisor's messages, research iterations, and coordinating parallel researchers.\n",
    "\n",
    "### Researcher State (Bottom Level)\n",
    "Each individual researcher has their own message history, tool call iterations, and research findings.\n",
    "\n",
    "We also have structured outputs for tool calling:\n",
    "- **ConductResearch** - Tool for supervisor to delegate research to a sub-agent\n",
    "- **ResearchComplete** - Tool to signal research phase is done\n",
    "- **ClarifyWithUser** - Structured output for asking clarifying questions\n",
    "- **ResearchQuestion** - Structured output for the research brief\n",
    "\n",
    "Let's import these from our library: [`open_deep_library/state.py`](open_deep_library/state.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import state definitions from the library\n",
    "from open_deep_library.state import (\n",
    "    # Main workflow states\n",
    "    AgentState,           # Lines 65-72: Top-level agent state with messages, research_brief, notes, final_report\n",
    "    AgentInputState,      # Lines 62-63: Input state is just messages\n",
    "    \n",
    "    # Supervisor states\n",
    "    SupervisorState,      # Lines 74-81: Supervisor manages research delegation and iterations\n",
    "    \n",
    "    # Researcher states\n",
    "    ResearcherState,      # Lines 83-90: Individual researcher with messages and tool iterations\n",
    "    ResearcherOutputState, # Lines 92-96: Output from researcher (compressed research + raw notes)\n",
    "    \n",
    "    # Structured outputs for tool calling\n",
    "    ConductResearch,      # Lines 15-19: Tool for delegating research to sub-agents\n",
    "    ResearchComplete,     # Lines 21-22: Tool to signal research completion\n",
    "    ClarifyWithUser,      # Lines 30-41: Structured output for user clarification\n",
    "    ResearchQuestion,     # Lines 43-48: Structured output for research brief\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ❓ Question 1:\n",
    "\n",
    " Explain the interrelationships between the three states.  Why don't we just make a single huge state?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ✅ Answer: \n",
    "The agent states interacts with the user and orchestrates overall workflow and final output. The supervisor state manages the research strategy and invokes the researches in parallel. The researcher state executes that focused research tasks independently and returns status. The benefits of this approach are separating parallel execution of researcher async processes,role clarity, ease of maintenance and trouble shooting to find errors. Also, when we have a single huge state it becomes difficult to understand which part is executed can make user anxious without a clue to the user what is being executed and having to wait until the final response. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Utility Functions and Tools\n",
    "\n",
    "The system uses several key utilities:\n",
    "\n",
    "### Search Tools\n",
    "- **tavily_search** - Async web search with automatic summarization to stay within token limits\n",
    "- Supports Anthropic native web search and Tavily API\n",
    "\n",
    "### Reflection Tools\n",
    "- **think_tool** - Allows researchers to reflect on their progress and plan next steps (ReAct pattern)\n",
    "\n",
    "### Helper Utilities\n",
    "- **get_all_tools** - Assembles the complete toolkit (search + MCP + reflection)\n",
    "- **get_today_str** - Provides current date context for research\n",
    "- Token limit handling utilities for graceful degradation\n",
    "\n",
    "These are defined in [`open_deep_library/utils.py`](open_deep_library/utils.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import utility functions and tools from the library\n",
    "from open_deep_library.utils import (\n",
    "    # Search tool - Lines 43-136: Tavily search with automatic summarization\n",
    "    tavily_search,\n",
    "    \n",
    "    # Reflection tool - Lines 219-244: Strategic thinking tool for ReAct pattern\n",
    "    think_tool,\n",
    "    \n",
    "    # Tool assembly - Lines 569-597: Get all configured tools\n",
    "    get_all_tools,\n",
    "    \n",
    "    # Date utility - Lines 872-879: Get formatted current date\n",
    "    get_today_str,\n",
    "    \n",
    "    # Supporting utilities for error handling\n",
    "    get_api_key_for_model,          # Lines 892-914: Get API keys from config or env\n",
    "    is_token_limit_exceeded,         # Lines 665-701: Detect token limit errors\n",
    "    get_model_token_limit,           # Lines 831-846: Look up model's token limit\n",
    "    remove_up_to_last_ai_message,    # Lines 848-866: Truncate messages for retry\n",
    "    anthropic_websearch_called,      # Lines 607-637: Detect Anthropic native search usage\n",
    "    openai_websearch_called,         # Lines 639-658: Detect OpenAI native search usage\n",
    "    get_notes_from_tool_calls,       # Lines 599-601: Extract notes from tool messages\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ❓ Question 2:  \n",
    "\n",
    "What are the advantages and disadvantages of importing these components instead of including them in the notebook?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ✅ Answer:\n",
    "#### Advantages: \n",
    "1.Module and Clean code is easy to read and maintain\n",
    "\n",
    "2.Easy to test, debug and modify single unit components\n",
    "\n",
    "3.Production ready code enabling parallel development and deployment\n",
    "\n",
    "4.Makes Utils reusable tools as packages\n",
    "\n",
    "5.Adhering Software engineering best practices like SOLID & DRY\n",
    "\n",
    "#### Disadvantages:\n",
    "1.It is complex for beginners, there is a learning curve if not from software development\n",
    "\n",
    "2.Bugs in the utils can impact every project that is used ( single point of failure)\n",
    "\n",
    "3.Need to know dependency management for imports and version conflict errors\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Configuration System\n",
    "\n",
    "The configuration system controls:\n",
    "\n",
    "### Research Behavior\n",
    "- **allow_clarification** - Whether to ask clarifying questions before research\n",
    "- **max_concurrent_research_units** - How many parallel researchers can run (default: 5)\n",
    "- **max_researcher_iterations** - How many times supervisor can delegate research (default: 6)\n",
    "- **max_react_tool_calls** - Tool call limit per researcher (default: 10)\n",
    "\n",
    "### Model Configuration\n",
    "- **research_model** - Model for research and supervision (we'll use Anthropic)\n",
    "- **compression_model** - Model for synthesizing findings\n",
    "- **final_report_model** - Model for writing the final report\n",
    "- **summarization_model** - Model for summarizing web search results\n",
    "\n",
    "### Search Configuration\n",
    "- **search_api** - Which search API to use (ANTHROPIC, TAVILY, or NONE)\n",
    "- **max_content_length** - Character limit before summarization\n",
    "\n",
    "Defined in [`open_deep_library/configuration.py`](open_deep_library/configuration.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import configuration from the library\n",
    "from open_deep_library.configuration import (\n",
    "    Configuration,    # Lines 38-247: Main configuration class with all settings\n",
    "    SearchAPI,        # Lines 11-17: Enum for search API options (ANTHROPIC, TAVILY, NONE)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Prompt Templates\n",
    "\n",
    "The system uses carefully engineered prompts for each phase:\n",
    "\n",
    "### Phase 1: Clarification\n",
    "**clarify_with_user_instructions** - Analyzes if the research scope is clear or needs clarification\n",
    "\n",
    "### Phase 2: Research Brief\n",
    "**transform_messages_into_research_topic_prompt** - Converts user messages into a detailed research brief\n",
    "\n",
    "### Phase 3: Supervisor\n",
    "**lead_researcher_prompt** - System prompt for the supervisor that manages delegation strategy\n",
    "\n",
    "### Phase 4: Researcher\n",
    "**research_system_prompt** - System prompt for individual researchers conducting focused research\n",
    "\n",
    "### Phase 5: Compression\n",
    "**compress_research_system_prompt** - Prompt for synthesizing research findings without losing information\n",
    "\n",
    "### Phase 6: Final Report\n",
    "**final_report_generation_prompt** - Comprehensive prompt for writing the final report\n",
    "\n",
    "All prompts are defined in [`open_deep_library/prompts.py`](open_deep_library/prompts.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import prompt templates from the library\n",
    "from open_deep_library.prompts import (\n",
    "    clarify_with_user_instructions,                    # Lines 3-41: Ask clarifying questions\n",
    "    transform_messages_into_research_topic_prompt,     # Lines 44-77: Generate research brief\n",
    "    lead_researcher_prompt,                            # Lines 79-136: Supervisor system prompt\n",
    "    research_system_prompt,                            # Lines 138-183: Researcher system prompt\n",
    "    compress_research_system_prompt,                   # Lines 186-222: Research compression prompt\n",
    "    final_report_generation_prompt,                    # Lines 228-308: Final report generation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Node Functions - The Building Blocks\n",
    "\n",
    "Now let's look at the node functions that make up our graph. We'll import them from the library and understand what each does.\n",
    "\n",
    "### The Complete Research Workflow\n",
    "\n",
    "The workflow consists of 8 key nodes organized into 3 subgraphs:\n",
    "\n",
    "1. **Main Graph Nodes:**\n",
    "   - `clarify_with_user` - Entry point that checks if clarification is needed\n",
    "   - `write_research_brief` - Transforms user input into structured research brief\n",
    "   - `final_report_generation` - Synthesizes all research into final report\n",
    "\n",
    "2. **Supervisor Subgraph Nodes:**\n",
    "   - `supervisor` - Lead researcher that plans and delegates\n",
    "   - `supervisor_tools` - Executes supervisor's tool calls (delegation, reflection)\n",
    "\n",
    "3. **Researcher Subgraph Nodes:**\n",
    "   - `researcher` - Individual researcher conducting focused research\n",
    "   - `researcher_tools` - Executes researcher's tool calls (search, reflection)\n",
    "   - `compress_research` - Synthesizes researcher's findings\n",
    "\n",
    "All nodes are defined in [`open_deep_library/deep_researcher.py`](open_deep_library/deep_researcher.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node 1: clarify_with_user\n",
    "\n",
    "**Purpose:** Analyzes user messages and asks clarifying questions if the research scope is unclear.\n",
    "\n",
    "**Key Steps:**\n",
    "1. Check if clarification is enabled in configuration\n",
    "2. Use structured output to analyze if clarification is needed\n",
    "3. If needed, end with a clarifying question for the user\n",
    "4. If not needed, proceed to research brief with verification message\n",
    "\n",
    "**Implementation:** [`open_deep_library/deep_researcher.py` lines 60-115](open_deep_library/deep_researcher.py#L60-L115)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the clarify_with_user node\n",
    "from open_deep_library.deep_researcher import clarify_with_user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node 2: write_research_brief\n",
    "\n",
    "**Purpose:** Transforms user messages into a structured research brief for the supervisor.\n",
    "\n",
    "**Key Steps:**\n",
    "1. Use structured output to generate detailed research brief from messages\n",
    "2. Initialize supervisor with system prompt and research brief\n",
    "3. Set up supervisor messages with proper context\n",
    "\n",
    "**Why this matters:** A well-structured research brief helps the supervisor make better delegation decisions.\n",
    "\n",
    "**Implementation:** [`open_deep_library/deep_researcher.py` lines 118-175](open_deep_library/deep_researcher.py#L118-L175)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the write_research_brief node\n",
    "from open_deep_library.deep_researcher import write_research_brief"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node 3: supervisor\n",
    "\n",
    "**Purpose:** Lead research supervisor that plans research strategy and delegates to sub-researchers.\n",
    "\n",
    "**Key Steps:**\n",
    "1. Configure model with three tools:\n",
    "   - `ConductResearch` - Delegate research to a sub-agent\n",
    "   - `ResearchComplete` - Signal that research is done\n",
    "   - `think_tool` - Strategic reflection before decisions\n",
    "2. Generate response based on current context\n",
    "3. Increment research iteration count\n",
    "4. Proceed to tool execution\n",
    "\n",
    "**Decision Making:** The supervisor uses `think_tool` to reflect before delegating research, ensuring thoughtful decomposition of the research question.\n",
    "\n",
    "**Implementation:** [`open_deep_library/deep_researcher.py` lines 178-223](open_deep_library/deep_researcher.py#L178-L223)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the supervisor node (from supervisor subgraph)\n",
    "from open_deep_library.deep_researcher import supervisor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node 4: supervisor_tools\n",
    "\n",
    "**Purpose:** Executes the supervisor's tool calls, including strategic thinking and research delegation.\n",
    "\n",
    "**Key Steps:**\n",
    "1. Check exit conditions:\n",
    "   - Exceeded maximum iterations\n",
    "   - No tool calls made\n",
    "   - `ResearchComplete` called\n",
    "2. Process `think_tool` calls for strategic reflection\n",
    "3. Execute `ConductResearch` calls in parallel:\n",
    "   - Spawn researcher subgraphs for each delegation\n",
    "   - Limit to `max_concurrent_research_units` (default: 5)\n",
    "   - Gather all results asynchronously\n",
    "4. Aggregate findings and return to supervisor\n",
    "\n",
    "**Parallel Execution:** This is where the magic happens - multiple researchers work simultaneously on different aspects of the research question.\n",
    "\n",
    "**Implementation:** [`open_deep_library/deep_researcher.py` lines 225-349](open_deep_library/deep_researcher.py#L225-L349)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the supervisor_tools node\n",
    "from open_deep_library.deep_researcher import supervisor_tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node 5: researcher\n",
    "\n",
    "**Purpose:** Individual researcher that conducts focused research on a specific topic.\n",
    "\n",
    "**Key Steps:**\n",
    "1. Load all available tools (search, MCP, reflection)\n",
    "2. Configure model with tools and researcher system prompt\n",
    "3. Generate response with tool calls\n",
    "4. Increment tool call iteration count\n",
    "\n",
    "**ReAct Pattern:** Researchers use `think_tool` to reflect after each search, deciding whether to continue or provide their answer.\n",
    "\n",
    "**Available Tools:**\n",
    "- Search tools (Tavily or Anthropic native search)\n",
    "- `think_tool` for strategic reflection\n",
    "- `ResearchComplete` to signal completion\n",
    "- MCP tools (if configured)\n",
    "\n",
    "**Implementation:** [`open_deep_library/deep_researcher.py` lines 365-424](open_deep_library/deep_researcher.py#L365-L424)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the researcher node (from researcher subgraph)\n",
    "from open_deep_library.deep_researcher import researcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node 6: researcher_tools\n",
    "\n",
    "**Purpose:** Executes the researcher's tool calls, including searches and strategic reflection.\n",
    "\n",
    "**Key Steps:**\n",
    "1. Check early exit conditions (no tool calls, native search used)\n",
    "2. Execute all tool calls in parallel:\n",
    "   - Search tools fetch and summarize web content\n",
    "   - `think_tool` records strategic reflections\n",
    "   - MCP tools execute external integrations\n",
    "3. Check late exit conditions:\n",
    "   - Exceeded `max_react_tool_calls` (default: 10)\n",
    "   - `ResearchComplete` called\n",
    "4. Continue research loop or proceed to compression\n",
    "\n",
    "**Error Handling:** Safely handles tool execution errors and continues with available results.\n",
    "\n",
    "**Implementation:** [`open_deep_library/deep_researcher.py` lines 435-509](open_deep_library/deep_researcher.py#L435-L509)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the researcher_tools node\n",
    "from open_deep_library.deep_researcher import researcher_tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node 7: compress_research\n",
    "\n",
    "**Purpose:** Compresses and synthesizes research findings into a concise, structured summary.\n",
    "\n",
    "**Key Steps:**\n",
    "1. Configure compression model\n",
    "2. Add compression instruction to messages\n",
    "3. Attempt compression with retry logic:\n",
    "   - If token limit exceeded, remove older messages\n",
    "   - Retry up to 3 times\n",
    "4. Extract raw notes from tool and AI messages\n",
    "5. Return compressed research and raw notes\n",
    "\n",
    "**Why Compression?** Researchers may accumulate lots of tool outputs and reflections. Compression ensures:\n",
    "- All important information is preserved\n",
    "- Redundant information is deduplicated\n",
    "- Content stays within token limits for the final report\n",
    "\n",
    "**Token Limit Handling:** Gracefully handles token limit errors by progressively truncating messages.\n",
    "\n",
    "**Implementation:** [`open_deep_library/deep_researcher.py` lines 511-585](open_deep_library/deep_researcher.py#L511-L585)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the compress_research node\n",
    "from open_deep_library.deep_researcher import compress_research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node 8: final_report_generation\n",
    "\n",
    "**Purpose:** Generates the final comprehensive research report from all collected findings.\n",
    "\n",
    "**Key Steps:**\n",
    "1. Extract all notes from completed research\n",
    "2. Configure final report model\n",
    "3. Attempt report generation with retry logic:\n",
    "   - If token limit exceeded, truncate findings by 10%\n",
    "   - Retry up to 3 times\n",
    "4. Return final report or error message\n",
    "\n",
    "**Token Limit Strategy:**\n",
    "- First retry: Use model's token limit × 4 as character limit\n",
    "- Subsequent retries: Reduce by 10% each time\n",
    "- Graceful degradation with helpful error messages\n",
    "\n",
    "**Report Quality:** The prompt guides the model to create well-structured reports with:\n",
    "- Proper headings and sections\n",
    "- Inline citations\n",
    "- Comprehensive coverage of all findings\n",
    "- Sources section at the end\n",
    "\n",
    "**Implementation:** [`open_deep_library/deep_researcher.py` lines 607-697](open_deep_library/deep_researcher.py#L607-L697)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the final_report_generation node\n",
    "from open_deep_library.deep_researcher import final_report_generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: Graph Construction - Putting It All Together\n",
    "\n",
    "The system is organized into three interconnected graphs:\n",
    "\n",
    "### 1. Researcher Subgraph (Bottom Level)\n",
    "Handles individual focused research on a specific topic:\n",
    "```\n",
    "START → researcher → researcher_tools → compress_research → END\n",
    "               ↑            ↓\n",
    "               └────────────┘ (loops until max iterations or ResearchComplete)\n",
    "```\n",
    "\n",
    "### 2. Supervisor Subgraph (Middle Level)\n",
    "Manages research delegation and coordination:\n",
    "```\n",
    "START → supervisor → supervisor_tools → END\n",
    "            ↑              ↓\n",
    "            └──────────────┘ (loops until max iterations or ResearchComplete)\n",
    "            \n",
    "supervisor_tools spawns multiple researcher_subgraphs in parallel\n",
    "```\n",
    "\n",
    "### 3. Main Deep Researcher Graph (Top Level)\n",
    "Orchestrates the complete research workflow:\n",
    "```\n",
    "START → clarify_with_user → write_research_brief → research_supervisor → final_report_generation → END\n",
    "                 ↓                                       (supervisor_subgraph)\n",
    "               (may end early if clarification needed)\n",
    "```\n",
    "\n",
    "Let's import the compiled graphs from the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pre-compiled graphs from the library\n",
    "from open_deep_library.deep_researcher import (\n",
    "    # Bottom level: Individual researcher workflow\n",
    "    researcher_subgraph,    # Lines 588-605: researcher → researcher_tools → compress_research\n",
    "    \n",
    "    # Middle level: Supervisor coordination\n",
    "    supervisor_subgraph,    # Lines 351-363: supervisor → supervisor_tools (spawns researchers)\n",
    "    \n",
    "    # Top level: Complete research workflow\n",
    "    deep_researcher,        # Lines 699-719: Main graph with all phases\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why This Architecture?\n",
    "\n",
    "### Advantages of Supervisor-Researcher Delegation\n",
    "\n",
    "1. **Dynamic Task Decomposition**\n",
    "   - Unlike section-based approaches with predefined structure, the supervisor can break down research based on the actual question\n",
    "   - Adapts to different types of research (comparisons, lists, deep dives, etc.)\n",
    "\n",
    "2. **Parallel Execution**\n",
    "   - Multiple researchers work simultaneously on different aspects\n",
    "   - Much faster than sequential section processing\n",
    "   - Configurable parallelism (1-20 concurrent researchers)\n",
    "\n",
    "3. **ReAct Pattern for Quality**\n",
    "   - Researchers use `think_tool` to reflect after each search\n",
    "   - Prevents excessive searching and improves search quality\n",
    "   - Natural stopping conditions based on information sufficiency\n",
    "\n",
    "4. **Flexible Tool Integration**\n",
    "   - Easy to add MCP tools for specialized research\n",
    "   - Supports multiple search APIs (Anthropic, Tavily)\n",
    "   - Each researcher can use different tool combinations\n",
    "\n",
    "5. **Graceful Token Limit Handling**\n",
    "   - Compression prevents token overflow\n",
    "   - Progressive truncation in final report generation\n",
    "   - Research can scale to arbitrary depths\n",
    "\n",
    "### Trade-offs\n",
    "\n",
    "- **Complexity:** More moving parts than section-based approach\n",
    "- **Cost:** Parallel researchers use more tokens (but faster)\n",
    "- **Unpredictability:** Research structure emerges dynamically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7: Running the Deep Researcher\n",
    "\n",
    "Now let's see the system in action! We'll use it to analyze a PDF document about how people use AI.\n",
    "\n",
    "### Setup\n",
    "\n",
    "We need to:\n",
    "1. Load the PDF document\n",
    "2. Configure the execution with Anthropic settings\n",
    "3. Run the research workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded PDF with 112460 characters\n",
      "First 500 characters:\n",
      "NBER WORKING PAPER SERIES\n",
      "HOW PEOPLE USE CHATGPT\n",
      "Aaron Chatterji\n",
      "Thomas Cunningham\n",
      "David J. Deming\n",
      "Zoe Hitzig\n",
      "Christopher Ong\n",
      "Carl Yan Shan\n",
      "Kevin Wadman\n",
      "Working Paper 34255\n",
      "http://www.nber.org/papers/w34255\n",
      "NATIONAL BUREAU OF ECONOMIC RESEARCH\n",
      "1050 Massachusetts Avenue\n",
      "Cambridge, MA 02138\n",
      "September 2025\n",
      "We acknowledge help and comments from Joshua Achiam, Hemanth Asirvatham, Ryan \n",
      "Beiermeister,  Rachel Brown, Cassandra Duchan Solis, Jason Kwon, Elliott Mokski, Kevin Rao, \n",
      "Harrison Satcher,  Gawe...\n"
     ]
    }
   ],
   "source": [
    "# Load the PDF document\n",
    "from pathlib import Path\n",
    "import PyPDF2\n",
    "\n",
    "def load_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"Load and extract text from PDF.\"\"\"\n",
    "    pdf_text = \"\"\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        for page in pdf_reader.pages:\n",
    "            pdf_text += page.extract_text() + \"\\n\\n\"\n",
    "    return pdf_text\n",
    "\n",
    "# Load the PDF about how people use AI\n",
    "pdf_path = \"data/howpeopleuseai.pdf\"\n",
    "pdf_content = load_pdf(pdf_path)\n",
    "\n",
    "print(f\"Loaded PDF with {len(pdf_content)} characters\")\n",
    "print(f\"First 500 characters:\\n{pdf_content[:500]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Graph ready for execution\n",
      "  (Note: The graph is pre-compiled from the library)\n"
     ]
    }
   ],
   "source": [
    "# Set up the graph with Anthropic configuration\n",
    "from IPython.display import Markdown, display\n",
    "import uuid\n",
    "\n",
    "# Note: deep_researcher is already compiled from the library\n",
    "# For this demo, we'll use it directly without additional checkpointing\n",
    "graph = deep_researcher\n",
    "\n",
    "print(\"✓ Graph ready for execution\")\n",
    "print(\"  (Note: The graph is pre-compiled from the library)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration for Anthropic\n",
    "\n",
    "We'll configure the system to use:\n",
    "- **Claude Sonnet 4** for all research, supervision, and report generation\n",
    "- **Tavily** for web search (you can also use Anthropic's native search)\n",
    "- **Moderate parallelism** (3 concurrent researchers)\n",
    "- **Clarification enabled** (will ask if research scope is unclear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Configuration ready\n",
      "  - Research Model: Claude Sonnet 4\n",
      "  - Max Concurrent Researchers: 3\n",
      "  - Max Iterations: 4\n",
      "  - Search API: Tavily\n"
     ]
    }
   ],
   "source": [
    "# Configure for Anthropic with moderate settings\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        # Model configuration - using Claude Sonnet 4 for everything\n",
    "        \"research_model\": \"anthropic:claude-sonnet-4-20250514\",\n",
    "        \"research_model_max_tokens\": 10000,\n",
    "        \n",
    "        \"compression_model\": \"anthropic:claude-sonnet-4-20250514\",\n",
    "        \"compression_model_max_tokens\": 8192,\n",
    "        \n",
    "        \"final_report_model\": \"anthropic:claude-sonnet-4-20250514\",\n",
    "        \"final_report_model_max_tokens\": 10000,\n",
    "        \n",
    "        \"summarization_model\": \"anthropic:claude-sonnet-4-20250514\",\n",
    "        \"summarization_model_max_tokens\": 8192,\n",
    "        \n",
    "        # Research behavior\n",
    "        \"allow_clarification\": True,\n",
    "        \"max_concurrent_research_units\": 1,  # 1 parallel researchers\n",
    "        \"max_researcher_iterations\": 2,      # Supervisor can delegate up to 2 times\n",
    "        \"max_react_tool_calls\": 3,           # Each researcher can make up to 3 tool calls\n",
    "        \n",
    "        # Search configuration\n",
    "        \"search_api\": \"tavily\",  # Using Tavily for web search\n",
    "        \"max_content_length\": 50000,\n",
    "        \n",
    "        # Thread ID for this conversation\n",
    "        \"thread_id\": str(uuid.uuid4())\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"✓ Configuration ready\")\n",
    "print(f\"  - Research Model: Claude Sonnet 4\")\n",
    "print(f\"  - Max Concurrent Researchers: 3\")\n",
    "print(f\"  - Max Iterations: 4\")\n",
    "print(f\"  - Search API: Tavily\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute the Research\n",
    "\n",
    "Now let's run the research! We'll ask the system to analyze the PDF and provide insights about how people use AI.\n",
    "\n",
    "The workflow will:\n",
    "1. **Clarify** - Check if the request is clear (may skip if obvious)\n",
    "2. **Research Brief** - Transform our request into a structured brief\n",
    "3. **Supervisor** - Plan research strategy and delegate to researchers\n",
    "4. **Parallel Research** - Multiple researchers gather information simultaneously\n",
    "5. **Compression** - Each researcher synthesizes their findings\n",
    "6. **Final Report** - All findings combined into comprehensive report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting research workflow...\n",
      "\n",
      "\n",
      "============================================================\n",
      "Node: clarify_with_user\n",
      "============================================================\n",
      "\n",
      "I have sufficient information to proceed with analyzing the NBER working paper \"How People Use ChatGPT.\" You've provided the PDF content and requested specific insights about:\n",
      "\n",
      "1. Main findings about how people are using AI\n",
      "2. Most common use cases \n",
      "3. Trends and patterns emerging from the data\n",
      "\n",
      "Based on the document provided, I can see this is a comprehensive study covering ChatGPT usage from November 2022 through July 2025, with detailed analysis of work vs. non-work usage, conversation topics, and user demographics. I will now begin analyzing this research paper to extract the key insights you requested.\n",
      "\n",
      "============================================================\n",
      "Node: write_research_brief\n",
      "============================================================\n",
      "\n",
      "Research Brief Generated:\n",
      "I have a PDF document containing the NBER Working Paper \"How People Use ChatGPT\" by Aaron Chatterji, Thomas Cunningham, David J. Deming, Zoe Hitzig, Christopher Ong, Carl Yan Shan, and Kevin Wadman (Working Paper 34255, September 2025). I need you to analyze this specific document and provide comprehensive insights about: (1) What are the main findings about how people are using AI/ChatGPT according to this research paper, (2) What are the most common use cases identified in the study, and (3) W...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Summarization timed out after 60 seconds, returning original content\n",
      "WARNING:root:Summarization timed out after 60 seconds, returning original content\n",
      "WARNING:root:Summarization timed out after 60 seconds, returning original content\n",
      "WARNING:root:Summarization timed out after 60 seconds, returning original content\n",
      "WARNING:root:Summarization timed out after 60 seconds, returning original content\n",
      "WARNING:root:Summarization timed out after 60 seconds, returning original content\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Node: research_supervisor\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Node: final_report_generation\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "FINAL REPORT GENERATED\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# How People Use ChatGPT: Comprehensive Analysis of NBER Working Paper 34255\n",
       "\n",
       "## Introduction and Study Scope\n",
       "\n",
       "The NBER Working Paper 34255 titled \"How People Use ChatGPT\" represents the most comprehensive study to date on ChatGPT usage patterns, conducted by researchers from OpenAI, Duke University, and Harvard University. Published in September 2025, this groundbreaking research documents ChatGPT's unprecedented growth from its November 2022 launch through July 2025, when it had been adopted by approximately 10% of the world's adult population—roughly 700 million users processing 18 billion messages weekly [1][3].\n",
       "\n",
       "The study's scale is remarkable: by June 2025, ChatGPT was processing more than 2.6 billion messages per day, or over 30,000 messages per second [1]. This represents a 5.8x increase in total message volume over just one year, while user growth was 3.2x, indicating that users are engaging more intensively with the technology as they gain experience [1].\n",
       "\n",
       "## Unprecedented Adoption and Growth Patterns\n",
       "\n",
       "ChatGPT achieved the fastest application adoption in history, reaching 1 million users in just 5 days after launch and 100 million weekly active users within less than one year [1][2]. The platform's growth has been exponential, with weekly active users doubling every 7-8 months and reaching over 750 million users by September 2025 [1].\n",
       "\n",
       "The research reveals that all user cohorts have increased their usage significantly, particularly beginning in late 2024 and early 2025. Even the earliest adopters from Q1 2023, after experiencing some decline through 2024, were sending 40% more messages per day by July 2025 than they did two years earlier. More recent users who signed up in Q3 and Q4 2024 nearly doubled their daily message volume within less than a year [1].\n",
       "\n",
       "## Work vs. Non-Work Usage Evolution\n",
       "\n",
       "One of the study's most significant findings concerns the dramatic shift in how ChatGPT is used. While both work-related and non-work-related messages have grown continuously, non-work usage has expanded much faster. The data shows a clear trend: non-work-related messages increased from 53% in June 2024 to 73% by June 2025, while work-related usage correspondingly decreased from 47% to 27% over the same period [1][3][6].\n",
       "\n",
       "This shift is primarily driven by changing usage patterns within existing user cohorts rather than changes in the composition of new users [6]. The finding challenges common assumptions about AI's primary value proposition, suggesting that while economic analysis has focused heavily on productivity gains in paid work, the impact on non-work activities (home production) may be on a similar or even larger scale [6].\n",
       "\n",
       "Work usage remains more prevalent among educated users in highly-paid professional occupations, but the overall trend indicates ChatGPT's expanding role in personal and recreational activities [2][3][6].\n",
       "\n",
       "## Most Common Use Cases: The Big Three\n",
       "\n",
       "The research identifies that nearly 80% of all ChatGPT usage falls into three dominant categories, representing the core value propositions of the platform [2][3][6][8]:\n",
       "\n",
       "### Practical Guidance (29% of usage)\n",
       "This is the single most common use case, maintaining consistent usage at roughly 29% of all interactions. Practical Guidance encompasses tutoring and teaching activities (10.2% of all messages), how-to advice on various topics (8.5% of all messages), and creative ideation. Education emerges as a major application, with 36% of Practical Guidance messages being requests for tutoring or teaching, while another 30% consists of general how-to advice [6][8].\n",
       "\n",
       "### Seeking Information (24% of usage, up from 14% in July 2024)\n",
       "This category has shown remarkable growth and includes searching for information about people, current events, products, and recipes. The research suggests this represents a very close substitute for traditional web search engines, indicating ChatGPT's growing role in information discovery and retrieval [2][3][6][9].\n",
       "\n",
       "### Writing (24% of usage, down from 36% in July 2024)\n",
       "Despite some decline in relative share, Writing remains a major use case encompassing automated production of emails, documents, and communications, as well as editing, critiquing, summarizing, and translating existing text. Notably, about two-thirds of all Writing messages involve modifying user-provided text rather than creating entirely new content from scratch [2]. Writing dominates work-related tasks, accounting for 40% of work-related messages in June 2025 [2][6][8].\n",
       "\n",
       "## User Intent Classifications: Asking, Doing, Expressing\n",
       "\n",
       "The study introduces an innovative taxonomy classifying messages by user intent, revealing important patterns in how people interact with AI:\n",
       "\n",
       "**Asking (49% of messages)** represents users seeking information or clarification to inform decisions, corresponding to problem-solving models of knowledge work. These messages consistently receive higher quality ratings from both automated classifiers and direct user feedback [6].\n",
       "\n",
       "**Doing (40% of messages)** involves users wanting to produce specific outputs or perform particular tasks, corresponding to traditional task-based work models. For work-related messages specifically, about 56% are classified as Doing, with nearly three-quarters of those being Writing tasks [6].\n",
       "\n",
       "**Expressing (11% of messages)** encompasses users expressing views or feelings without seeking information or action. This category has grown from under 8% in July 2024 to 13.8% by June 2025, indicating increasing use of ChatGPT for social-emotional purposes [6].\n",
       "\n",
       "## Demographic Trends and Shifts\n",
       "\n",
       "### Gender Demographics\n",
       "The research documents a dramatic narrowing of the gender gap in ChatGPT usage. Early adopters were heavily male-skewed, with about 80% having typically masculine first names in the initial months. However, by June 2025, this had reversed to 48% masculine and 52% feminine first names, indicating successful adoption across gender lines [1][6][7][10].\n",
       "\n",
       "### Age Demographics\n",
       "Young users dominate ChatGPT usage, with nearly half of all adult messages sent by users under 26, and more than 45% of all users being under 25 [2][6][10]. However, age-related usage patterns vary significantly: work-related messages comprised only 23% for users under 26, increasing with age, though users aged 66 or older sent just 16% of their messages for work purposes [12].\n",
       "\n",
       "### Geographic Growth\n",
       "The study reveals particularly strong growth in low- and middle-income countries, with usage increasing by 5-6x in middle-income countries compared to 3x growth in the richest countries [1][9][10]. This geographic diversification suggests ChatGPT is becoming a global tool rather than remaining concentrated in wealthy nations.\n",
       "\n",
       "## Programming and Technical Usage: Lower Than Expected\n",
       "\n",
       "Contrary to widespread assumptions about AI's primary use in technical fields, computer programming accounts for only 4.2% of all ChatGPT messages [6][8][11]. This is significantly lower than other platforms like Claude, where programming represents 33% of work-related conversations [6][11]. \n",
       "\n",
       "Technical Help overall, including programming (4.2%), mathematical calculations (3%), and data analysis (0.4%), has actually declined from 12% of all usage in July 2024 to around 5% a year later [6]. This finding challenges the tech community's focus on coding capabilities and suggests broader, more diverse applications drive adoption [11].\n",
       "\n",
       "## Workplace Integration and O*NET Mapping\n",
       "\n",
       "The research mapped ChatGPT usage to workplace activities using the Occupational Information Network (O*NET), revealing that about 58% of work-related messages are associated with two broad categories: obtaining, documenting, and interpreting information; and making decisions, giving advice, solving problems, and thinking creatively [6].\n",
       "\n",
       "Remarkably, the work activities associated with ChatGPT usage show consistency across very different occupations. \"Getting Information\" and \"Making Decisions and Solving Problems\" rank in the top five message frequencies across management, business, STEM, administrative, and sales occupations [6][12]. This suggests ChatGPT's value transcends specific professional domains, providing universal support for knowledge work activities.\n",
       "\n",
       "## Social-Emotional Usage: Smaller Than Expected\n",
       "\n",
       "Despite speculation about AI companionship, the research finds relatively limited usage for social-emotional purposes. Only 1.9% of messages relate to relationships and personal reflection, while just 0.4% involve games and role-play [6]. This contrasts sharply with other estimates suggesting therapy and companionship were prevalent use cases for generative AI [6].\n",
       "\n",
       "## Economic Value and Decision Support\n",
       "\n",
       "The study concludes that ChatGPT's primary economic value lies in decision support, particularly important in knowledge-intensive jobs [2][6]. Rather than simply automating tasks, ChatGPT serves as a decision-support system, helping users weigh options, choose better words, and interpret information [8].\n",
       "\n",
       "The research indicates the highest satisfaction and fastest growth occur in advisory roles such as tutoring, giving advice, and helping people think through problems rather than pure task completion [13]. This aligns with the finding that almost half of all usage falls into Practical Guidance or Seeking Information categories [6].\n",
       "\n",
       "## Quality and User Satisfaction Patterns\n",
       "\n",
       "\"Asking\" messages consistently receive higher quality ratings compared to \"Doing\" messages, both from automated satisfaction classifiers and direct user feedback [6]. This suggests users find ChatGPT most valuable when seeking guidance and information rather than task execution, reinforcing the platform's strength as an advisory tool rather than a pure automation solution.\n",
       "\n",
       "## Implications for AI Development and Society\n",
       "\n",
       "The research reveals ChatGPT's evolution from a primarily work-focused tool to a broader platform supporting diverse personal and professional needs. The dominance of non-work usage (73% by June 2025) suggests the economic impact of AI extends far beyond workplace productivity into home production and personal development.\n",
       "\n",
       "The study's finding that information-seeking has grown dramatically (from 14% to 24% of usage) indicates ChatGPT is increasingly competing with traditional search engines, while the consistent strength of practical guidance suggests unique value in advisory and educational contexts that traditional search cannot provide.\n",
       "\n",
       "## Methodology and Privacy Protection\n",
       "\n",
       "The research employed rigorous privacy-preserving methodologies, with no research team member ever viewing actual user message content. Instead, automated classifiers analyzed messages stripped of personally identifiable information through an internal tool called Privacy Filter, with all analysis conducted using Data Clean Rooms and aggregated outputs over limited categories [1][6]. This approach enables comprehensive usage analysis while maintaining user privacy—a critical consideration for studying sensitive communication data.\n",
       "\n",
       "### Sources\n",
       "\n",
       "[1] How People Use ChatGPT - by David Deming - Forked Lightning: https://forklightning.substack.com/p/how-people-use-chatgpt\n",
       "\n",
       "[2] Number of ChatGPT Users (October 2025) - Exploding Topics: https://explodingtopics.com/blog/chatgpt-users\n",
       "\n",
       "[3] 73% of ChatGPT Uses Are Non-Work Related, OpenAI Study Finds: https://tech.co/news/chatgpt-uses-non-work-related-openai-study\n",
       "\n",
       "[4] How People Are Really Using ChatGPT - Mike Jeffs: https://mikejeffs.com/blog/how-people-are-really-using-chatgpt/\n",
       "\n",
       "[5] How People Actually Use ChatGPT — What 1.5M Conversations Tell Us: https://medium.com/@adnanmasood/how-people-actually-use-chatgpt-what-1-5m-conversations-tell-us-about-the-next-decade-of-software-ea603212b458\n",
       "\n",
       "[6] How People Use ChatGPT | OpenAI (PDF): https://cdn.openai.com/pdf/a253471f-8260-40c6-a2cc-aa93fe9f142e/economic-research-chatgpt-usage-paper.pdf\n",
       "\n",
       "[7] How People Use ChatGPT - Edward Conard: https://www.edwardconard.com/macro-roundup/as-of-july-2025-chatgpt-was-used-at-least-weekly-by-10-of-the-global-adult-population-70-of-usage-was-non-work-related-work-usage-was-most-common-for-educated-users-in-high-pay-professional-occupa/?view=detail\n",
       "\n",
       "[8] ChatGPT usage: 80% of conversations fall into three categories: https://www.linkedin.com/posts/aaron-ronnie-chatterji_nearly-80-of-all-chatgpt-conversations-fall-activity-7374916815124131841-VSwt\n",
       "\n",
       "[9] ChatGPT Study: 1 In 4 Conversations Now Seek Information: https://www.searchenginejournal.com/chatgpt-study-1-in-4-conversations-now-seek-information/556104/\n",
       "\n",
       "[10] ChatGPT Study Reveals 1 in 4 Conversations Now Seek Information: https://www.ravi-gupta.com/news/chatgpt-study-1-in-4-conversations-now-seek-information/\n",
       "\n",
       "[11] How people use ChatGPT: NBER study reveals usage patterns: https://www.linkedin.com/posts/julienchaumond_chatgpt-usage-breakdown-aka-how-activity-7374435587233329153-yYBp\n",
       "\n",
       "[12] Over 70% ChatGPT Interactions Are Non-Work, Says OpenAI: https://www.medianama.com/2025/09/223-over-70-chatgpt-interactions-non-work-guidance-openai/\n",
       "\n",
       "[13] The Impact of ChatGPT Exposure on User Interactions: https://pmc.ncbi.nlm.nih.gov/articles/PMC11952273/"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Research workflow completed!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Create our research request with PDF context\n",
    "research_request = f\"\"\"\n",
    "I have a PDF document about how people use AI. Please analyze this document and provide insights about:\n",
    "\n",
    "1. What are the main findings about how people are using AI?\n",
    "2. What are the most common use cases?\n",
    "3. What trends or patterns emerge from the data?\n",
    "\n",
    "Here's the PDF content:\n",
    "\n",
    "{pdf_content[:10000]}  # First 10k chars to stay within limits\n",
    "\n",
    "...[content truncated for context window]\n",
    "\"\"\"\n",
    "\n",
    "# Execute the graph\n",
    "async def run_research():\n",
    "    \"\"\"Run the research workflow and display results.\"\"\"\n",
    "    print(\"Starting research workflow...\\n\")\n",
    "    \n",
    "    async for event in graph.astream(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": research_request}]},\n",
    "        config,\n",
    "        stream_mode=\"updates\"\n",
    "    ):\n",
    "        # Display each step\n",
    "        for node_name, node_output in event.items():\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Node: {node_name}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            if node_name == \"clarify_with_user\":\n",
    "                if \"messages\" in node_output:\n",
    "                    last_msg = node_output[\"messages\"][-1]\n",
    "                    print(f\"\\n{last_msg.content}\")\n",
    "            \n",
    "            elif node_name == \"write_research_brief\":\n",
    "                if \"research_brief\" in node_output:\n",
    "                    print(f\"\\nResearch Brief Generated:\")\n",
    "                    print(f\"{node_output['research_brief'][:500]}...\")\n",
    "            \n",
    "            elif node_name == \"supervisor\":\n",
    "                print(f\"\\nSupervisor planning research strategy...\")\n",
    "                if \"supervisor_messages\" in node_output:\n",
    "                    last_msg = node_output[\"supervisor_messages\"][-1]\n",
    "                    if hasattr(last_msg, 'tool_calls') and last_msg.tool_calls:\n",
    "                        print(f\"Tool calls: {len(last_msg.tool_calls)}\")\n",
    "                        for tc in last_msg.tool_calls:\n",
    "                            print(f\"  - {tc['name']}\")\n",
    "            \n",
    "            elif node_name == \"supervisor_tools\":\n",
    "                print(f\"\\nExecuting supervisor's tool calls...\")\n",
    "                if \"notes\" in node_output:\n",
    "                    print(f\"Research notes collected: {len(node_output['notes'])}\")\n",
    "            \n",
    "            elif node_name == \"final_report_generation\":\n",
    "                if \"final_report\" in node_output:\n",
    "                    print(f\"\\n\" + \"=\"*60)\n",
    "                    print(\"FINAL REPORT GENERATED\")\n",
    "                    print(\"=\"*60 + \"\\n\")\n",
    "                    display(Markdown(node_output[\"final_report\"]))\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Research workflow completed!\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "# Run the research\n",
    "await run_research()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Output\n",
    "\n",
    "Let's break down what happened:\n",
    "\n",
    "### Phase 1: Clarification\n",
    "The system checked if your request was clear. Since you provided a PDF and specific questions, it likely proceeded without clarification.\n",
    "\n",
    "### Phase 2: Research Brief\n",
    "Your request was transformed into a detailed research brief that guides the supervisor's delegation strategy.\n",
    "\n",
    "### Phase 3: Supervisor Delegation\n",
    "The supervisor analyzed the brief and decided how to break down the research:\n",
    "- Used `think_tool` to plan strategy\n",
    "- Called `ConductResearch` multiple times to delegate to parallel researchers\n",
    "- Each delegation specified a focused research topic\n",
    "\n",
    "### Phase 4: Parallel Research\n",
    "Multiple researchers worked simultaneously:\n",
    "- Each researcher used web search tools to gather information\n",
    "- Used `think_tool` to reflect after each search\n",
    "- Decided when they had enough information\n",
    "- Compressed their findings into clean summaries\n",
    "\n",
    "### Phase 5: Final Report\n",
    "All research findings were synthesized into a comprehensive report with:\n",
    "- Well-structured sections\n",
    "- Inline citations\n",
    "- Sources listed at the end\n",
    "- Balanced coverage of all findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 🏗️ Activity #1: Try Different Configurations\n",
    "\n",
    "You can experiment with different settings to see how they affect the research.  You may select three or more of the following settings (or invent your own experiments) and describe the results.\n",
    "\n",
    "### Increase Parallelism\n",
    "```python\n",
    "\"max_concurrent_research_units\": 10  # More researchers working simultaneously\n",
    "```\n",
    "\n",
    "### Deeper Research\n",
    "```python\n",
    "\"max_researcher_iterations\": 8   # Supervisor can delegate more times\n",
    "\"max_react_tool_calls\": 15      # Each researcher can search more\n",
    "```\n",
    "\n",
    "### Use Anthropic Native Search\n",
    "```python\n",
    "\"search_api\": \"anthropic\"  # Use Claude's built-in web search\n",
    "```\n",
    "\n",
    "### Disable Clarification\n",
    "```python\n",
    "\"allow_clarification\": False  # Skip clarification phase\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Configuration ready\n",
      "  - Research Model: Claude Sonnet 4\n",
      "  - Max Concurrent Researchers: 1\n",
      "  - Max Iterations: 2\n",
      "  - Search API: Anthropic\n"
     ]
    }
   ],
   "source": [
    "# Configure for Anthropic with moderate settings\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        # Model configuration - using Claude Sonnet 4 for everything\n",
    "        \"research_model\": \"anthropic:claude-sonnet-4-20250514\",\n",
    "        \"research_model_max_tokens\": 5000,\n",
    "        \n",
    "        \"compression_model\": \"anthropic:claude-sonnet-4-20250514\",\n",
    "        \"compression_model_max_tokens\": 8192,\n",
    "        \n",
    "        \"final_report_model\": \"anthropic:claude-sonnet-4-20250514\",\n",
    "        \"final_report_model_max_tokens\": 10000,\n",
    "        \n",
    "        \"summarization_model\": \"anthropic:claude-sonnet-4-20250514\",\n",
    "        \"summarization_model_max_tokens\": 8192,\n",
    "        \n",
    "        # Research behavior\n",
    "        \"allow_clarification\": True,  # Tried changing from True to False; got error TypeError: argument of type 'NoneType' is not iterable\n",
    "        \"max_concurrent_research_units\": 1,  # 1 parallel researchers \n",
    "        \"max_researcher_iterations\": 2,      # Supervisor can delegate up to 2 times\n",
    "        \"max_react_tool_calls\": 5,           # Each researcher can make up to 5 tool calls\n",
    "        \n",
    "        # Search configuration\n",
    "        \"search_api\": \"anthropic\",  # Use Claude's built-in web search\n",
    "        \"max_content_length\": 5000,\n",
    "        \n",
    "        # Thread ID for this conversation\n",
    "        \"thread_id\": str(uuid.uuid4())\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"✓ Configuration ready\")\n",
    "print(f\"  - Research Model: Claude Sonnet 4\")\n",
    "print(f\"  - Max Concurrent Researchers: 1\")\n",
    "print(f\"  - Max Iterations: 2\")\n",
    "print(f\"  - Search API: Anthropic\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting research workflow...\n",
      "\n",
      "\n",
      "============================================================\n",
      "Node: clarify_with_user\n",
      "============================================================\n",
      "\n",
      "I have sufficient information to proceed with analyzing the NBER working paper \"How People Use ChatGPT\" by Chatterji et al. (2025). I understand you want me to focus on three key areas: (1) main findings about how people are using AI, (2) the most common use cases, and (3) trends or patterns that emerge from the data. I will now begin analyzing the provided PDF content to extract these insights and provide you with a comprehensive report on ChatGPT usage patterns.\n",
      "\n",
      "============================================================\n",
      "Node: write_research_brief\n",
      "============================================================\n",
      "\n",
      "Research Brief Generated:\n",
      "I need you to analyze the NBER working paper \"How People Use ChatGPT\" by Aaron Chatterji, Thomas Cunningham, David J. Deming, Zoe Hitzig, Christopher Ong, Carl Yan Shan, and Kevin Wadman (Working Paper No. 34255, September 2025) and provide comprehensive insights on three specific dimensions: (1) What are the main findings about how people are using AI/ChatGPT based on this research, (2) What are the most common use cases identified in the study, and (3) What trends or patterns emerge from the d...\n",
      "\n",
      "============================================================\n",
      "Node: research_supervisor\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Node: final_report_generation\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "FINAL REPORT GENERATED\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# How People Use ChatGPT: Comprehensive Analysis of NBER Research Findings\n",
       "\n",
       "The NBER working paper \"How People Use ChatGPT\" by Chatterji, Cunningham, Deming, Hitzig, Ong, Shan, and Wadman represents the first large-scale empirical study examining how people actually use AI chatbots in practice. This groundbreaking research provides unprecedented insights into user behavior patterns, usage trends, and the evolving role of AI in both work and personal contexts.\n",
       "\n",
       "## Overview of Research Scope and Methodology\n",
       "\n",
       "The study analyzed ChatGPT usage patterns from May 2024 to July 2025, examining 1.58 million messages from users through a privacy-preserving automated pipeline [1][2]. The research team employed multiple LLM-based classifiers to categorize messages while ensuring no human ever read the actual content of user messages [2][6]. The study was approved by Harvard IRB and utilized a secure data clean room protocol to protect user privacy while enabling comprehensive analysis [5][6].\n",
       "\n",
       "The research focused exclusively on consumer plans (Free, Plus, and Pro), excluding business and enterprise accounts, and covered messages from users who had not opted out of data sharing [2][6]. This represents one of the most comprehensive analyses of AI chatbot usage patterns to date.\n",
       "\n",
       "## Main Findings About AI/ChatGPT Usage Patterns\n",
       "\n",
       "### Unprecedented Growth and Global Adoption\n",
       "\n",
       "ChatGPT has achieved the fastest global technology diffusion in recorded history. Launched as a \"research preview\" on November 30, 2022, it reached one million registered users within five days [5]. By July 2025, ChatGPT had grown to serve 700 million users sending 18 billion messages weekly, representing approximately 10% of the global adult population [1][2][3].\n",
       "\n",
       "The scale of this adoption is staggering when compared to other major technologies. ChatGPT reached 100 million weekly active users in less than one year, and by September 2025, had grown to more than 750 million weekly active users [5]. The number of users has been doubling every 7-8 months, while message volume has grown even faster at 5.8x in the last year compared to 3.2x growth in users [5].\n",
       "\n",
       "### Shift from Work to Personal Use\n",
       "\n",
       "One of the most significant findings concerns the evolution of how people use ChatGPT. The research reveals a dramatic shift from work-related to personal applications over time. Non-work messages grew from 53% in mid-2024 to more than 70% by mid-2025 [1][4][8]. This trend represents a fundamental change in user behavior patterns.\n",
       "\n",
       "Critically, this shift is not primarily due to new types of users joining the platform, but rather reflects changing usage patterns within existing user cohorts [2]. As users become more familiar with ChatGPT's capabilities, they increasingly apply it to personal tasks and home production activities rather than just work-related functions.\n",
       "\n",
       "### Economic Impact Beyond Workplace Productivity\n",
       "\n",
       "The researchers emphasize that while most economic analysis of AI has focused on workplace productivity, the impact on activities outside of work appears to be on a similar or possibly larger scale [2]. This finding challenges conventional assumptions about AI's primary economic value and suggests that consumer applications may be equally important to professional ones.\n",
       "\n",
       "The study concludes that ChatGPT provides significant economic value through decision support, which is especially important in knowledge-intensive jobs, but extends far beyond traditional workplace boundaries [1][3][7].\n",
       "\n",
       "## Most Common Use Cases Identified\n",
       "\n",
       "### The Big Three: Practical Guidance, Information Seeking, and Writing\n",
       "\n",
       "The research identified three dominant use cases that collectively account for nearly 80% of all ChatGPT conversations [1][3][7]:\n",
       "\n",
       "**Practical Guidance** emerges as the single most common use case, encompassing activities like tutoring and teaching, how-to advice, and ideation [2][4]. This category represents the largest share of user interactions and demonstrates ChatGPT's role as a digital advisor across various domains.\n",
       "\n",
       "**Seeking Information** covers a broad range of queries about people, events, products, and recipes [4]. This use case positions ChatGPT as an alternative or complement to traditional search engines, offering more conversational and contextual responses.\n",
       "\n",
       "**Writing** represents a particularly significant category, especially for work-related tasks. The research found that writing dominates work-related applications, highlighting chatbots' unique ability to generate digital outputs compared to traditional search engines [1][3][7]. Writing represents approximately 40% of work-related messages [4][8].\n",
       "\n",
       "### Detailed Analysis of Writing Applications\n",
       "\n",
       "Within the writing category, the researchers discovered that about two-thirds of requests involve editing, critiquing, translating, or summarizing text that users provided, rather than generating entirely new content [8]. This finding suggests that users primarily view ChatGPT as a collaborative writing assistant rather than a replacement for human creativity.\n",
       "\n",
       "### Educational Applications\n",
       "\n",
       "Education stands out as a notable specialized use case, with tutoring requests accounting for about 10% of all ChatGPT activity [4]. This represents a significant application area that spans both personal learning and potentially work-related skill development.\n",
       "\n",
       "### Less Common but Notable Use Cases\n",
       "\n",
       "The study found that computer programming and self-expression represent relatively small shares of overall usage [1][3][7], despite receiving significant attention in public discussions about AI applications. This finding helps calibrate expectations about the relative importance of different AI use cases in practice.\n",
       "\n",
       "## Trends and Patterns Emerging from the Data\n",
       "\n",
       "### Demographic Evolution and Accessibility\n",
       "\n",
       "The research reveals significant demographic shifts in ChatGPT adoption over time. Early adopters were disproportionately male, but the gender gap has narrowed dramatically [1][7]. By January 2024, among users with identifiable names, 37% had typically feminine names, and by mid-2025, adoption patterns closely resembled the general adult population [3].\n",
       "\n",
       "Geographically, the study identified higher growth rates in lower-income countries [1][7], suggesting that ChatGPT is becoming increasingly accessible across different economic contexts globally.\n",
       "\n",
       "### Work Usage Patterns and Professional Applications\n",
       "\n",
       "Work-related usage shows distinct patterns based on education and occupation. The research found that work usage is most common among educated users in highly-paid professional occupations [1][7]. Professionals in higher-paid technical and knowledge-driven roles are significantly more likely to employ ChatGPT at work [8].\n",
       "\n",
       "The study utilized O*NET work activity classifications to categorize messages, mapping them to 332 Individual Work Activities (IWAs) and then aggregating to Generalized Work Activities (GWAs) [8]. This systematic approach revealed that ChatGPT applications in work contexts are heavily concentrated in knowledge work and creative tasks.\n",
       "\n",
       "### Intensification of Usage Over Time\n",
       "\n",
       "A critical trend identified in the research is the intensification of usage among existing users. Message volume has grown faster than user volume (5.8x versus 3.2x), indicating that ChatGPT users are engaging with the technology more intensively as they gain experience with it [5]. This suggests that familiarity breeds increased reliance rather than reduced novelty-driven usage.\n",
       "\n",
       "### Comparative Context with Other Technologies\n",
       "\n",
       "The research provides important context by comparing ChatGPT's growth to other major technologies. By June 2025, ChatGPT users were sending more than 2.6 billion messages per day (more than 30,000 messages per second), compared to an estimated 14 billion daily Google searches [5]. However, Google search took eight years to reach 1 billion daily searches after its 1999 public launch, while ChatGPT achieved comparable scale in a fraction of that time [5].\n",
       "\n",
       "### Privacy-Preserving Research Methods\n",
       "\n",
       "The study demonstrates important methodological innovations in analyzing large-scale user data while preserving privacy. The researchers used automated classifiers validated against the WildChat public dataset, with human reviewers helping to fine-tune classification prompts until achieving high fidelity with human judgment [5]. This approach enables comprehensive analysis of user behavior without compromising individual privacy.\n",
       "\n",
       "## Implications for Understanding AI Adoption\n",
       "\n",
       "The NBER research reveals that ChatGPT usage patterns differ significantly from initial expectations and public discourse about AI applications. The shift toward personal and home production uses, the dominance of practical guidance and information-seeking over programming and creative applications, and the rapid global diffusion across demographic groups all challenge conventional assumptions about how people integrate AI into their lives.\n",
       "\n",
       "The study's finding that usage intensifies over time rather than declining after initial novelty suggests that ChatGPT is becoming an integral tool for many users rather than a temporary fascination. The economic implications extend far beyond workplace productivity to encompass significant value creation in personal and domestic contexts.\n",
       "\n",
       "These findings provide crucial empirical grounding for policy discussions, business strategies, and further research into AI's societal impact. They demonstrate that understanding AI's true impact requires examining actual usage patterns rather than relying on theoretical projections or anecdotal evidence.\n",
       "\n",
       "### Sources\n",
       "\n",
       "[1] How People Use ChatGPT | NBER: https://www.nber.org/papers/w34255\n",
       "[2] NBER WORKING PAPER SERIES HOW PEOPLE USE CHATGPT: https://www.nber.org/system/files/working_papers/w34255/w34255.pdf\n",
       "[3] How people are using ChatGPT | OpenAI: https://openai.com/index/how-people-are-using-chatgpt/\n",
       "[4] How People Really Use ChatGPT: Findings from NBER Research: https://techmaniacs.com/2025/09/15/how-people-really-use-chatgpt-findings-from-nber-research/\n",
       "[5] How People Use ChatGPT - by David Deming - Forked Lightning: https://forklightning.substack.com/p/how-people-use-chatgpt\n",
       "[6] How People Use ChatGPT: https://cdn.openai.com/pdf/a253471f-8260-40c6-a2cc-aa93fe9f142e/economic-research-chatgpt-usage-paper.pdf\n",
       "[7] How People Use ChatGPT: https://www.pw.hks.harvard.edu/post/how-people-use-chatgpt\n",
       "[8] How People Actually Use ChatGPT: https://medium.com/towards-artificial-intelligence/how-people-actually-use-chatgpt-2790df683c00"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Research workflow completed!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Create our research request with PDF context\n",
    "research_request = f\"\"\"\n",
    "I have a PDF document about how people use AI. Please analyze this document and provide insights about:\n",
    "\n",
    "1. What are the main findings about how people are using AI?\n",
    "2. What are the most common use cases?\n",
    "3. What trends or patterns emerge from the data?\n",
    "\n",
    "Here's the PDF content:\n",
    "\n",
    "{pdf_content[:2000]}  # First 2k chars to stay within limits\n",
    "\n",
    "...[content truncated for context window]\n",
    "\"\"\"\n",
    "\n",
    "# Execute the graph\n",
    "async def run_research():\n",
    "    \"\"\"Run the research workflow and display results.\"\"\"\n",
    "    print(\"Starting research workflow...\\n\")\n",
    "    \n",
    "    async for event in graph.astream(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": research_request}]},\n",
    "        config,\n",
    "        stream_mode=\"updates\"\n",
    "    ):\n",
    "        # Display each step\n",
    "        for node_name, node_output in event.items():\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Node: {node_name}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            if node_name == \"clarify_with_user\":\n",
    "                if \"messages\" in node_output:\n",
    "                    last_msg = node_output[\"messages\"][-1]\n",
    "                    print(f\"\\n{last_msg.content}\")\n",
    "            \n",
    "            elif node_name == \"write_research_brief\":\n",
    "                if \"research_brief\" in node_output:\n",
    "                    print(f\"\\nResearch Brief Generated:\")\n",
    "                    print(f\"{node_output['research_brief'][:500]}...\")\n",
    "            \n",
    "            elif node_name == \"supervisor\":\n",
    "                print(f\"\\nSupervisor planning research strategy...\")\n",
    "                if \"supervisor_messages\" in node_output:\n",
    "                    last_msg = node_output[\"supervisor_messages\"][-1]\n",
    "                    if hasattr(last_msg, 'tool_calls') and last_msg.tool_calls:\n",
    "                        print(f\"Tool calls: {len(last_msg.tool_calls)}\")\n",
    "                        for tc in last_msg.tool_calls:\n",
    "                            print(f\"  - {tc['name']}\")\n",
    "            \n",
    "            elif node_name == \"supervisor_tools\":\n",
    "                print(f\"\\nExecuting supervisor's tool calls...\")\n",
    "                if \"notes\" in node_output:\n",
    "                    print(f\"Research notes collected: {len(node_output['notes'])}\")\n",
    "            \n",
    "            elif node_name == \"final_report_generation\":\n",
    "                if \"final_report\" in node_output:\n",
    "                    print(f\"\\n\" + \"=\"*60)\n",
    "                    print(\"FINAL REPORT GENERATED\")\n",
    "                    print(\"=\"*60 + \"\\n\")\n",
    "                    display(Markdown(node_output[\"final_report\"]))\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Research workflow completed!\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "# Run the research\n",
    "await run_research()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### Architecture Benefits\n",
    "1. **Dynamic Decomposition** - Research structure emerges from the question, not predefined\n",
    "2. **Parallel Efficiency** - Multiple researchers work simultaneously\n",
    "3. **ReAct Quality** - Strategic reflection improves search decisions\n",
    "4. **Scalability** - Handles token limits gracefully through compression\n",
    "5. **Flexibility** - Easy to add new tools and capabilities\n",
    "\n",
    "### When to Use This Pattern\n",
    "- **Complex research questions** that need multi-angle investigation\n",
    "- **Comparison tasks** where parallel research on different topics is beneficial\n",
    "- **Open-ended exploration** where structure should emerge dynamically\n",
    "- **Time-sensitive research** where parallel execution speeds up results\n",
    "\n",
    "### When to Use Section-Based Instead\n",
    "- **Highly structured reports** with predefined format requirements\n",
    "- **Template-based content** where sections are always the same\n",
    "- **Sequential dependencies** where later sections depend on earlier ones\n",
    "- **Budget constraints** where token efficiency is critical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ✅ Answer:\n",
    "##### Observations:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "### Extend the System\n",
    "1. **Add MCP Tools** - Integrate specialized tools for your domain\n",
    "2. **Custom Prompts** - Modify prompts for specific research types\n",
    "3. **Different Models** - Try different Claude versions or mix models\n",
    "4. **Persistence** - Use a real database for checkpointing instead of memory\n",
    "\n",
    "### Learn More\n",
    "- [LangGraph Documentation](https://langchain-ai.github.io/langgraph/)\n",
    "- [Open Deep Research Repo](https://github.com/langchain-ai/open_deep_research)\n",
    "- [Anthropic Claude Documentation](https://docs.anthropic.com/)\n",
    "- [Tavily Search API](https://tavily.com/)\n",
    "\n",
    "### Deploy\n",
    "- Use LangGraph Cloud for production deployment\n",
    "- Add proper error handling and logging\n",
    "- Implement rate limiting and cost controls\n",
    "- Monitor research quality and costs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
